[[["7a049726-0dfa-4a01-9621-6eb3cfc77318",{"pageContent":"000\n001\n002\n003\n004\n005\n006\n007\n008\n009\n010\n011\n012\n013\n014\n015\n016\n017\n018\n019\n020\n021\n022\n023\n024\n025\n026\n027\n028\n029\n030\n031\n032\n033\n034\n035\n036\n037\n038\n039\n040\n041\n042\n043\n044\n045\nAUTHOR(S):1\nProactive Defense Against Localized\nUniversal Adversary Attacks\nBMVC 2023 Submission # 130\nAbstract\nThis paper addresses the vulnerability of deep learning models, particularly convo-\nlutional neural networks (CNNs), to adversarial attacks and presents a proactive training\ntechnique designed to counter them. We introduce a novel volumization algorithm, which\ntransforms  2D  images  into  3D  volumetric  representations.   When  combined  with  3D\nconvolution and deep curriculum learning optimization (CLO), it significantly improves\nthe immunity of models against localized universal attacks by up to 40%.  We evaluate\nour proposed approach using contemporary CNN architectures and the modified Cana-\ndian Institute for Advanced Research (CIFAR-10 and CIFAR-100) [11] and ImageNet[6]","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1,"to":59}}}}],["d786d900-bce1-4a2a-b4c0-8cdc88e73fa1",{"pageContent":"our proposed approach using contemporary CNN architectures and the modified Cana-\ndian Institute for Advanced Research (CIFAR-10 and CIFAR-100) [11] and ImageNet[6]\ndatasets, showcasing accuracy improvements over previous techniques. The results indi-\ncate that the combination of the volumetric input and curriculum learning holds signifi-\ncant promise for mitigating adversarial attacks without necessitating adversary training.\n1    Introduction\nThe security of any machine learning model is assessed in terms of the goals and capabilities\nassociated with adversary attacks. Algorithmically crafted perturbations, even if minuscule,\ncan be exploited as directives to manipulate classification outcomes [2, 4, 16, 19].  Attacks\ncan be classified as black-box or white-box[3], depending on the attacker’s access to and\nknowledge of the model’s information,  which includes its architecture,  parameters,  train-\ning data, weights, and more.  In a white-box attack, the attacker has complete access to the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":59,"to":70}}}}],["bda1ea7e-9ebc-476b-add8-5ae0489315ff",{"pageContent":"knowledge of the model’s information,  which includes its architecture,  parameters,  train-\ning data, weights, and more.  In a white-box attack, the attacker has complete access to the\nnetwork’s information, while a black-box attack is characterized by the absence of knowl-\nedge regarding the model’s internal configuration.  Occasionally, a gray-box attack can be\ngenerated by employing a generative model, enabling the creation of adversarial examples\nwithout access to the victim model. Localized adversarial attacks [15] exploit spatial invari-\nance of CNN-based image classifiers by introducing minimal perturbations to deceive the\nmodel into producing incorrect classifications.  These attacks are usually constrained to a\nsmall contiguous portion of the image and are image-agnostic (or universal) attacks.\nIn this paper, we introduce a new training methodology (Figure 1) designed to fortify\nCNNs against localized attacks.   Our primary approach incorporates deep curriculum op-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":70,"to":80}}}}],["f2b19fe4-a705-41a8-871f-49dc141355c0",{"pageContent":"In this paper, we introduce a new training methodology (Figure 1) designed to fortify\nCNNs against localized attacks.   Our primary approach incorporates deep curriculum op-\ntimization [7] and a volumization algorithm.   We employ an information-theoretic repre-\nsentation of an image along with optimization procedure that merges batch-based curricu-\nlum learning (CL), patch aggregate loss (PAL) function,  and 3D convolution to train and\nproactively defend against effective localized attacks such as one-pixel and adversary patch\nattacks.\n© 2023. The copyright of this document resides with its authors.\nIt may be distributed unchanged freely in print or electronic forms.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":80,"to":88}}}}],["761d2248-e48d-45eb-9d36-b8a1ee84cef6",{"pageContent":"046\n047\n048\n049\n050\n051\n052\n053\n054\n055\n056\n057\n058\n059\n060\n061\n062\n063\n064\n065\n066\n067\n068\n069\n070\n071\n072\n073\n074\n075\n076\n077\n078\n079\n080\n081\n082\n083\n084\n085\n086\n087\n088\n089\n090\n091\n2AUTHOR(S):\nFigure 1: This figure illustrates the proposed training process. The process starts with gener-\nating a syllabus (input path) for the batch. It then volumizes each image, followed by feature\nextraction using 3D CNNs. The patch aggregate loss function is applied to optimize param-\neters.\nSpatial invariance property of CNNs enables learning of features in an image regard-\nless of their position within the image.  This is achieved using convolutions on small, non-\noverlapping regions of the image, essentially learning local features. This property can also\nbe exploited by localized attacks, as even tiny perturbations in specific regions can lead to\nincorrect classifications. To counteract this vulnerability, we designed a preprocessing algo-\nrithm that divides the input image into non-overlapping patches and generates a 3D volume","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":90,"to":146}}}}],["abeb6ad7-760b-4467-8f24-e83b99524095",{"pageContent":"incorrect classifications. To counteract this vulnerability, we designed a preprocessing algo-\nrithm that divides the input image into non-overlapping patches and generates a 3D volume\nof  these  patches  for  training.   By  focusing  on  smaller  regions,  the  algorithm  enables  the\nmodel to learn localized features more effectively. This approach helps the models to proac-\ntively  counteract  localized  attacks  while  maintaining  accurate  classification  performance.\nWe present results that show our method better preserves classification accuracy and has a\nhigher defense success rate than adversary training-based defense approaches. In summary,\nthis paper’s contributions are:(a)A novel training methodology that employs volumization\nby dividing an image into patches and creating a 3D volume, combined with a curriculum\nlearning approach for adaptive training.(b)Demonstrate the effectiveness of our proposed","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":146,"to":155}}}}],["48a16b4f-18b6-4ac0-b897-501b8e0a1dd5",{"pageContent":"by dividing an image into patches and creating a 3D volume, combined with a curriculum\nlearning approach for adaptive training.(b)Demonstrate the effectiveness of our proposed\napproach in defending against localized attacks through extensive experimental validation.\nOur results indicate that models trained using this technique exhibit improved robustness\n(compared to existing approaches) against N-pixel and patch attacks while maintaining high\ngeneralization performance.\nIn section 2, we present related work.  Section 3 details problem formalism - highlight-\ning the attack and defense objectives.  In section 4 we present the proposed methodology,\nwhile the following section 5 presents experimental results and discussion.  Finally we give\nconcluding remarks in section 6.\n2    Related Work\nN-Pixel Attack Defenses.Su et al.  uses a technique which generates adversarial examples\nby perturbing only one or more pixels [19], has proven to be difficult to defend.  To date,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":155,"to":167}}}}],["7c8bf096-b2bd-4388-a0fb-e7863bec796d",{"pageContent":"092\n093\n094\n095\n096\n097\n098\n099\n100\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\nAUTHOR(S):3\nthe most successful defense against this attack is a method presented by Chen et al.  [4].\nThe authors propose Patch Selection Denoiser (PSD) that removes few of the potentially\nattacked pixels in the whole image. At the cost of image degradation, the authors demonstrate\nsuccessful defense against one-pixel attacks.   Similarly,  Liu et al.   [13] proposed a three-\nstep image reconstruction algorithm to remove attacked pixels.  The authors report defense\nsuccess rate for N-pixel attack forNchosen from (1, 15).\nAdversary Patch Defenses.Defenses for patch attacks are typically viewed as a detec-\ntion problem [14, 22, 23]. Once the patch’s location is detected, the suspected region would\nbe either masked or in-painted to mitigate the adversarial influence on the image.  Hayes","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":169,"to":224}}}}],["4abe6f95-d31f-46a8-98f9-064341c94d60",{"pageContent":"tion problem [14, 22, 23]. Once the patch’s location is detected, the suspected region would\nbe either masked or in-painted to mitigate the adversarial influence on the image.  Hayes\net al.  [9] first proposed DW (Digital Watermarking), a defense against adversarial patches,\ninspired by the procedure of digital watermarking removal [12].  A saliency map of the im-\nage was constructed to help remove small holes and mask the adversarial image, blocking\nadversarial perturbations. This was an empirical defense with no guarantee against adaptive\nadversaries. Naseer et al. [15] proposed LGS (Local Gradient Smoothing) to suppress highly\nactivated and perturbed regions in the image without affecting salient objects.  Specifically,\nthe irregular gradients were regularized in the image before being passed to a deep neural\nnetwork (DNN) model for inference. LGS could achieve robustness with a minimal drop in\nclean accuracy because it was based on local region processing in contrast to the global pro-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":224,"to":234}}}}],["516be376-4e64-467e-bd9a-e2f82afb97f6",{"pageContent":"network (DNN) model for inference. LGS could achieve robustness with a minimal drop in\nclean accuracy because it was based on local region processing in contrast to the global pro-\ncessing on the whole image as done by its counterparts. Chen et al. [5] proposed Jujutsu to\ndetect and mitigate robust and universal adversarial patch attacks by leveraging the attacks’\nlocalized nature via image inpainting. A modified saliency map was used to detect the pres-\nence of highly active perturbed regions, which helped to place suspicious extracted regions\nin the least salient regions of the preserved image and avoid occlusion with main objects in\nthe image.  Jujutsu showed a better performance than other empirical defenses in terms of\nboth robust accuracy and low false-positive rate (FPR), across datasets, patches of various\nshapes, and attacks that targeted different classes.  Gittings et al.  [8] proposed training time","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":234,"to":243}}}}],["42e2a2a3-f9d6-43f8-9b24-1e380a8343c9",{"pageContent":"both robust accuracy and low false-positive rate (FPR), across datasets, patches of various\nshapes, and attacks that targeted different classes.  Gittings et al.  [8] proposed training time\ndefense, which involves injecting adversarial vaccines into the input data to improve the ro-\nbustness of the network against adversarial patch attacks.  The approach achieved a defense\nsuccess rate of 91.6% against adversarial patch attacks on the CIFAR-10 dataset, and was\nevaluated against several state-of-the-art adversarial patch attacks\nThe main tenet of our approach is to proactively mitigate the impact of adversarial at-\ntacks without prior knowledge of the attacks.  Rather than relying on detection and removal\ntechniques or adversary training, our method is designed to immunize models by enhancing\ntheir inherent robustness.\n3    Problem Formulation\nLetfbe a CNN classifier, represented asf:R\nM\n→R\nc\n, trained on a dataset{D= (x\ni\n,y\ni\n):\nx\nn\n∈X,y\nn","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":243,"to":266}}}}],["03974d5a-c19c-4183-a02b-7c6dd4688030",{"pageContent":"their inherent robustness.\n3    Problem Formulation\nLetfbe a CNN classifier, represented asf:R\nM\n→R\nc\n, trained on a dataset{D= (x\ni\n,y\ni\n):\nx\nn\n∈X,y\nn\n∈Y,∀n}with input-label pair set(X,Y), to map a source imagex(having width\nW, heightH, number of channelsC), to a set of probabilitiesf(x). Each probability inf(x)\nencodes the likelihood thatxbelongs to one of thekclassesc\ni\n∈Y. Here|X|=Mis the total\nnumber of samples in the dataset and|Y|=cis the total number of classes.\nAttack Objective:  An attack on an imagexinvolves adding a perturbationr∈R\nm\nto\nx,  causing the maximized class probabilities to differ between the original and perturbed\nimages. i.e.,\nargmax\ni\n(f\ni\n(x+r)̸=argmax\ni\n(f\ni\n(x)).(1)","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":266,"to":300}}}}],["ffe03d6f-3c40-41d9-8af0-9fae8670b99a",{"pageContent":"138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n4AUTHOR(S):\nThese types of attacks can be categorized as targeted or untargeted. In a targeted attack,\nthe adversarial imagex\n′\n=x+ris generated to induce the classifier to assignx\n′\nto a\nspecific target classc\nt\n∈Y, wherec\nt\n̸=argmax\ni\n(f\ni\n(x)). The perturbationris selected such\nthatargmax\ni\n(f\ni\n(x′)) =c\nt\n. Conversely, in an untargeted attack, the perturbation is crafted to\ncause the classifier to assignx\n′\nto any incorrect class without a particular target. In this case,\nthe perturbationris chosen to satisfyargmax\ni\n(f\ni\n(x\n′\n))̸=argmax\ni\n(f\ni\n(x))without imposing\nadditional constraints on the target class. In this study, we only focus on untargeted attacks.\nDefence Objective:  The defense objective is to train a modelgthat is robust to untar-\ngeted universal adversarial image attacks without sacrificing the accuracy of the classifier on","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":302,"to":387}}}}],["9c607ef6-0002-4b83-8f35-dc04e7e01d09",{"pageContent":"Defence Objective:  The defense objective is to train a modelgthat is robust to untar-\ngeted universal adversarial image attacks without sacrificing the accuracy of the classifier on\nthe original dataset. Formally, the objective is to findgthat minimizes the following loss:\nmin\ng\n1\n|\nD\n|\n∑\n(x,y)∈D\nmax\nr∈R\nL(g(x+r),y)(2)\nwhereRis the set of possible adversary perturbations, andLis a loss function used to train\nthe modelg.  The objective is to minimize the maximum loss over all possible adversarial\nexamplesx\n′\ngenerated by any allowable perturbation inr∈R. In this study,Ris constrained\nto be a set of localized attacks. Localized attacks are characterized by the property that theL\n2\nnorm of the perturbation vectorr, denoted by||r||, is much smaller than that of the original\ninput imagex, denoted by||x||( i.e.||r||≪||x||). These attacks modify only a small subset\nof pixels that are confined to a localized region of the image.\n4    Method","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":387,"to":411}}}}],["39be5a3b-1467-424e-9334-c629379b16f1",{"pageContent":"input imagex, denoted by||x||( i.e.||r||≪||x||). These attacks modify only a small subset\nof pixels that are confined to a localized region of the image.\n4    Method\nOur aim is to develop a defended classifier,g, that inherently defends against localized at-\ntacks.  To this end we propose a proactive defense approach that incorporates the following\nkey steps:(1)Volumization - for each image in the batch,  we convert it to a 3D volume\n(section 4.1);(2)3D Convolution - we modify contemporary CNN architectures for 3D con-\nvolution, which enables the model to extract features from the 3D volumes.  Details of the\nmodifications are discussed in section 4.2;(3)Deep curriculum learning optimization - for\neach batch taken from the datasetD, we generate a syllabus that determines the input or-\nder of the samples in the batch.  This curriculum learning approach helps the model learn\nprogressively from simpler to more complex samples (section 4.3).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":411,"to":422}}}}],["a1db1a54-1e9a-4b9b-acf2-93357b03b352",{"pageContent":"der of the samples in the batch.  This curriculum learning approach helps the model learn\nprogressively from simpler to more complex samples (section 4.3).\nThis approach ensure that the model remains resilient to perturbations and maintains ac-\ncurate classification and verification of both the original imagesxand adversarial imagesx\n′\n.\nBy converting images to 3D volumes, our method can capture and preserve spatial infor-\nmation that can help in defending against localized attacks.  The deep curriculum learning\noptimization further improves the model’s ability to recognize and learn from complex pat-\nterns, which aids in the proactive defense. Figure 1 depicts the end-to-end training process.\n4.1    Volumization Algorithm\nThe volumization algorithm, is a pixel-preserving transformation operator denoted as a func-\ntion:\nV:(x,H,W,C)→\n\u0000\nx\n′\ni\n,p\nh\n,p\nw\n,C,N\n\u0001\n.(3)\nGiven  an  input  imagex∈Dof  shape(H, W, C),  the  algorithm  uses  a  configurable\nhyperparameter, patch sizeP(p\nh\n,p\nw\n)- wherep\nh\nandp\nw","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":422,"to":455}}}}],["3f647cf4-133e-4b1c-bf5b-3b9787f8bd04",{"pageContent":"tion:\nV:(x,H,W,C)→\n\u0000\nx\n′\ni\n,p\nh\n,p\nw\n,C,N\n\u0001\n.(3)\nGiven  an  input  imagex∈Dof  shape(H, W, C),  the  algorithm  uses  a  configurable\nhyperparameter, patch sizeP(p\nh\n,p\nw\n)- wherep\nh\nandp\nw\nare the height and width of each","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":455,"to":477}}}}],["024ddb2e-1463-4cac-8f5d-19ee68184318",{"pageContent":"184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n200\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\nAUTHOR(S):5\npatch - to splitxintoNnon-overlapping patchesx\n′\ni\nof sizePsatisfying the pixel conservation\ncondition:\nx=\nN−1\n[\ni=0\nx\n′\ni\n(4)\nThis states that the original imagexis equivalent to the union of all its extracted patches.\nHere,∪denotes the union operation. The algorithm extracts a list of patches and proceeds to\nrank each patches according to a prespecified metricm– a configurable hyperparameter. The\nchosen metric can be of type distance or standalone (Table 1). Ifmis a distance-based metric,\na reference patchP\nre f\nis selected, which can be user-defined or automatically determined by\nchoosing the patch with the lowest entropy.  On the other hand,mis considered standalone\nif it measures some characteristics of a given patch.  All patches are then ordered based on\ntheir individual metric scores or their distances from the reference patch. The orderingordis","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":479,"to":548}}}}],["d35b6a59-7bcc-450a-857b-756f4de7dbb4",{"pageContent":"if it measures some characteristics of a given patch.  All patches are then ordered based on\ntheir individual metric scores or their distances from the reference patch. The orderingordis\nuser-define configurable hyperparameter that can be either descending or ascending. Finally,\nthe ranked patches are stacked along the depth axis to create a 3D volumex\nv\n=V(x)of\nshape(p\nh\n,p\nw\n,C,N), whereN(depth of the volume) is the total number of patches. Refer\nto section 4.3 for detail on the ranking and ordering process, which is identical for both the\nvolumizer and CL when generating a syllabus for a batch.\n4.2    Architecture Modifications\nGiven a conventional CNN architecturef, designed to learn from 2D images, we perform the\nfollowing modifications to constructg- a 3D counterpart off. The modifications result in a\nsignificant but tolerable increase in the number of parameters for each model, with VGG16\nincreasing by 20,663,968 parameters, ResNet50 by 48,566,533 parameters, InceptionV3 by","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":548,"to":565}}}}],["e361c49a-5ee5-45f6-a16b-aaa885220af0",{"pageContent":"significant but tolerable increase in the number of parameters for each model, with VGG16\nincreasing by 20,663,968 parameters, ResNet50 by 48,566,533 parameters, InceptionV3 by\n44,252,266 parameters, and EfficientNetB0 by 10,569,436 parameters.\nInput Layer:  The input layer off,  denoted asI\n2D\n,  is designed to accept a 2D input\ndataxwith dimensionsH×W×Csuch thatI\n2D\n:x∈R\nH×W×C\n.  In order to extract features\nfrom the volumized images, the input layer of modelgis adjusted to be 3-dimensional,I\n3D\n,\nwhere the input to this layerx\nv\nis a 4D tensor with dimensionsp\nh\n×p\nw\n×C×N, such that\nI\n3D\n:x\nv\n∈R\np\nh\n×p\nw\n×C×N\n. In short hand notation, this modification can be represented as,\nf(I\n2D\n:x∈R\nH×W×C\n)→g(I\n3D\n:x\nv\n∈R\np\nh\n×p\nw\n×C×N\n)(5)\nConvolution Layers: In a CNN, convolution represents the interaction between an input\n(image or feature map) and a kernelK.Kis a small matrix that slides over the input data,\nperforming an element-wise multiplication and summation of the results to generate a feature","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":565,"to":614}}}}],["306f465a-09b6-4bdf-b874-9f0b1f2c5766",{"pageContent":"(image or feature map) and a kernelK.Kis a small matrix that slides over the input data,\nperforming an element-wise multiplication and summation of the results to generate a feature\nmap. In a 2D convolution, both the input data and the kernel are two-dimensional.\n(K∗x)(i,j) =\n∑\nm\n∑\nn\nK(m,n)x(i−m,j−n)(6)\nHere,Krepresents the 2D kernel,xrepresents the 2D input, and(i,j)are the coordinates in\nthe output feature map.  The summation is performed over all spatial dimensions(m,n)of\nthe 2D kernel.\nA 3D counterpart of the above operation is:\n(K\n3D\n∗x\nv\n)(i,j,k) =\n∑\nm\n′\n∑\nn\n′\n∑\np\n′\nK\n3D\n(m\n′\n,n\n′\n,p\n′\n)x\nv\n(i−m\n′\n,j−n\n′\n,k−p\n′\n)(7)","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":614,"to":657}}}}],["6108309e-7162-4f68-a83c-f3f98fa811f6",{"pageContent":"230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n6AUTHOR(S):\nwhereK\n3D\nrepresents the 3D kernel,x\nv\nis the 3D input data, and(i,j,k)are the coordinates in\nthe output feature map. The summation is performed over all spatial dimensions(m\n′\n,n\n′\n,p\n′\n)\nof the 3D kernel.  This modification enables the classifier to learn features from the volu-\nmized data by processing spatial information across height,  width,  and depth dimensions\nsimultaneously.\nNote that the optimal kernel size depends on the size of the individual patches within\nthe volume and the desired level of spatial information capture.  For example, iffconsists\nof 1x1, 3x3, and 5x5 2D convolution layers, we adjust these layers to be 1x1xN, 3x3xN,\nand 5x5xN 3D convolution layers, respectively.  Here,Nis the depth of the input volume\nx\nv\ncontaining patches of size(p\nw\n,p\nh\n).  To ensure compatibility, we enforce the constraint","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":659,"to":731}}}}],["b49d0d95-4040-44a9-b068-1b436d0b1fa9",{"pageContent":"and 5x5xN 3D convolution layers, respectively.  Here,Nis the depth of the input volume\nx\nv\ncontaining patches of size(p\nw\n,p\nh\n).  To ensure compatibility, we enforce the constraint\nthat the kernel size is much smaller than the size of the individual patches and that the kernel\noperates on each patch in the volume. That is,m\n′\n≪p\nw\n, andn\n′\n≪p\nh\n. The stride and padding\nvalues are also adjusted accordingly.\nPooling Layers: All pooling layers offare modified to handle the 3D volume represen-\ntation of the input data. Inf, the 2D pooling layers are denoted asP\n2D\n:\nP\n2D\n:R\n(H\nin\n×W\nin\n×C\nin\n)\n→R\n(H\nout\n×W\nout\n×C\nout\n)\n(8)\nwhereH\nin\n,W\nin\n, andC\nin\nrepresent the height, width, and number of channels of the input\nimage or feature maps, whileH\nout\n,W\nout\n, andC\nout\ndenote the height, width, and number of\nchannels of the output feature maps, respectively.\nTo effectively process volumized inputs, we replace the 2D pooling layers with 3D coun-\nterparts:\nP\n3D\n:R\n(p\nh\nin\n×p\nw\nin\n×C\nin\n×N)\n→R\n(p\nh\nout\n×p\nw\nout\n×C\nout\n×N)\n(9)\nwherep\nh\nin\n,p\nw\nin\n, andC\nin","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":731,"to":820}}}}],["3bbdb3cd-0f75-4f71-b099-60ae778ca0d1",{"pageContent":"To effectively process volumized inputs, we replace the 2D pooling layers with 3D coun-\nterparts:\nP\n3D\n:R\n(p\nh\nin\n×p\nw\nin\n×C\nin\n×N)\n→R\n(p\nh\nout\n×p\nw\nout\n×C\nout\n×N)\n(9)\nwherep\nh\nin\n,p\nw\nin\n, andC\nin\nrepresent the height, width, and number of channels of the input\nvolume, whilep\nh\nout\n,p\nw\nout\n, andC\nout\ndenote the height, width, and number of channels of the\noutput 3D volume, respectively.Nrepresents the number of patches.\nPooling layers reduce the spatial dimensions of the input data while preserving essen-\ntial features.  For example, 3D max pooling computes the maximum value of the elements\nwithin the pooling window, effectively preserving the most salient features while reducing\ndimension of the input.  In the case of 3D max pooling, the operation is performed not only\nacross height and width but also across depth, ensuring that the most important features in\neach patch are retained.\nThe inclusion of the depth dimension in 3D pooling allows for better preservation of spa-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":820,"to":870}}}}],["c722f7ea-41ee-45c6-a662-39f2b13ff611",{"pageContent":"each patch are retained.\nThe inclusion of the depth dimension in 3D pooling allows for better preservation of spa-\ntial relationships in the input data, thus preserving more contextual information during the\ndown-sampling process.  This has significant implications considering the model’s robust-\nness against adversarial attacks.  When using 2D pooling, the model may be more suscepti-\nble to adversarial perturbations that exploit weaknesses in the spatial relationships between\nimage regions.  Conversely, 3D pooling preserves the spatial relationships between regions,\nallowing the model to better understand and leverage the contextual information present in\nthe input image.  This is because 3D pooling operation is applied across not only width and\nheight, but also the depth dimension, which includes the different patches. This takes into ac-\ncount the spatial relationships between the patches, preserving more contextual information","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":870,"to":880}}}}],["f22104e9-252e-4a82-a9c4-49daaef835d6",{"pageContent":"height, but also the depth dimension, which includes the different patches. This takes into ac-\ncount the spatial relationships between the patches, preserving more contextual information\nduring the down-sampling process.  For instance, 3D max-pooling would select the maxi-\nmum value within its 3D pooling window, which spans across multiple patches, and retain\nthe spatial relationships between them.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":880,"to":884}}}}],["2ebc50ab-9b61-46b8-8230-068b79a3c4f5",{"pageContent":"276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n300\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\nAUTHOR(S):7\nFully Connected and Output Layers: We modifyf’s fully connected layer to beFC:\nR\n(M,N)\n→R\n(L,N)\n, whereMrepresents the number of input features,Ldenotes the number of\noutput features, andNis the total number of patches. This maps each patch’s input features\nto its respective output features.  Note that, to ensure compatibility with theFClayers, the\noutputs featuresMof the preceding layers must be reshaped or flattened.  The output layer\nfunction,O, maps output featuresLofFCtoknumber of classes:O:R\n(L,N)\n→R\n(k,N)\n. This\nmodification allowsgto map each patch’s feature to its respective class probabilities, thereby\nenabling patch-wise errorL\np\ncalculations during training.\nPatch Aggregate Loss (PAL) Functionis designed to enable backpropagation on indi-\nvidual patches.  During training, the loss for each patch is calculated separately.  The patch-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":886,"to":952}}}}],["11c399b2-2420-49fe-8ac3-c7313fe30771",{"pageContent":"Patch Aggregate Loss (PAL) Functionis designed to enable backpropagation on indi-\nvidual patches.  During training, the loss for each patch is calculated separately.  The patch-\nwise losses are then aggregated to obtain the overall loss for the image. Given the modified\noutputO, we compute PAL in two steps as:\n1.Patch-wise Error Calculation.  We first compute the loss for each patch separately\nusing a suitable loss functionL\np\n:R\n(N\n′\n,k)\n→R\n(N\n′\n)\n. Given a patchn∈ {0,1,...,N−\n1},  the patch-wise loss is calculated asL\np\n(y\nn\n,y),  wherey\nn\nrepresents the predicted\nclass probabilities of pathn, andydenotes the true class labels of the originals input.\n2.Loss Aggregation. At this step we combine the patch-wise losses to obtain the overall\nloss for the image. PAL is determined by summing up the individual patch-wise losses:\nPAL=\nN−1\n∑\nn=0\nL\np\n(y\nn\n,y)(10)\nUsing the sum of patch-wise losses directly emphasizes the importance of minimizing the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":952,"to":987}}}}],["aca81436-ca57-419c-91ec-18392f0bda61",{"pageContent":"loss for the image. PAL is determined by summing up the individual patch-wise losses:\nPAL=\nN−1\n∑\nn=0\nL\np\n(y\nn\n,y)(10)\nUsing the sum of patch-wise losses directly emphasizes the importance of minimizing the\nerror for each individual patch, driving the model to learn more robust features from each\npatch.  This increased emphasis on localized features results in a more robust model that is\nbetter equipped to counteract attacks.\n4.3    Training Methodology\nWe incorporate curriculum learning optimization (CLO) at a batch level to enhance the train-\ning process.  Given a batchB⊆D,we define a syllabusSas a functionS:B→B\n′\n,where\nB\n′\nis a reordered version of the original batchB.Sdescribes an input order of the samples\ninB\n′\nsuch that the learning process progresses from simpler to more complex samples as\nquantified by a concrete metricmtaken from Table 1.Ordering of samples for the batch\nMetricCategoryImplementation\nEntropy (H)standalone\n∑\ni∈χ,x∈T\nb\nx\n(i)log\nN\nb\nx\n(i)\nMutual Information (MI)distanceH(x\n1\n)+H(x\n2\n)−JE(x\n1\n,x\n2\n)","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":987,"to":1032}}}}],["78ce4b39-8271-49fd-a336-53dd798d4a1d",{"pageContent":"MetricCategoryImplementation\nEntropy (H)standalone\n∑\ni∈χ,x∈T\nb\nx\n(i)log\nN\nb\nx\n(i)\nMutual Information (MI)distanceH(x\n1\n)+H(x\n2\n)−JE(x\n1\n,x\n2\n)\nKL-Divergence (KL)distanceD\nk||L\n(x\n1\n,x\n2\n) =\n∑\ni\nx\n1\ni\nlog\nb\nx\n1\n(i)\nb\nx\n2\n(i)\nPeak Signal to Noise Ratio (PSNR)distancePSNR=20 log\n10\n(\nMAX\n√\nMSE\n)\nMax Norm (MN)distancex\n∞\n=max(x\n1\n,...,x\nn\n)\nJoint Entropy (JE)distanceJE(x\n1\n,x\n2\n) =\n∑\ni\nb\nx\n(i)logb\nx\n(i)\nMean Squared Error (MSE)distance\n1\nN\n2\n∑\nN\ni\n∑\nN\nj\n(x\n1\ni j\n−x\n2\ni j\n)\n2\nTable  1:  List  of  measures  used  in  this  study.   Given  samplesx,x\n1\n,x\n2\n∈Bwhereb\nx\nis\nnormalized histogram of pixel intensities andiis an index of a pixel value in the image’s\nvector.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1032,"to":1125}}}}],["5cecc1a4-fa1e-421b-83aa-bb84f6435201",{"pageContent":"322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n8AUTHOR(S):\nis done in the same way the volumizer algorithm orders patches to create a volume.  Given\nB={x\n1\n,x\n2\n,...,x\nn\n}, a batch ofnsamples (or a set of patches belonging tox), let SM(x\ni\n)be\nthe standalone metric value ofx\ni\nandDM(x\ni\n,r)be the distance metric value of the sample or\npatchx\ni\nwith respect to a reference image (or patch)r, respectively. We define order relations\nR\nSM\n⊆BandR\nDM\n⊆B, such that:\n(x\ni\n,x\nj\n)∈\n(\nR\nSM\ni f SM(x\ni\n)≤SM(x\nj\n)\nR\nDM\ni f DM(x\ni\n,r)≤DM(x\nj\n,r)\n(11)\nThus, the syllabus (or volumizer) algorithm transformsB(set of patches) into an ordered\noneB\n′\n:\nS\nSM\n(B) ={x\n′\n1\n,...,x\n′\nn\n},where(x\n′\ni\n,x\n′\nj\n)∈R\nSM\n}(12)\nS\nDM\n(B) ={x\n′\n1\n,...,x\n′\nn\n},where(x\n′\ni\n,x\n′\nj\n)∈R\nDM\n}(13)\nThis ordering ensures a progression from simpler to more complex samples based on the\nchosen metric during the learning process. By ordering the batch based on the complexity of","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1127,"to":1257}}}}],["4d01d6e9-2a07-425b-be51-c3b921142804",{"pageContent":"′\ni\n,x\n′\nj\n)∈R\nDM\n}(13)\nThis ordering ensures a progression from simpler to more complex samples based on the\nchosen metric during the learning process. By ordering the batch based on the complexity of\nimages as measured by a concrete metric, the training process adapts to the dataset’s inher-\nent structure, allowing the model to adjust its learning strategy as it encounters increasingly\ncomplex samples.  This is proven to result in better model performance and faster conver-\ngence [7].  In addition, this approach enables the models to becomes more robust against\nlow-level, localized attacks. Since the patches are ordered and positioned within the volume\nbased on their characteristics, adversarial perturbations in a specific patch are less likely to\naffect the overall understanding of the image by the model.\nFigure 2 presents the losses and success rates on CIFAR10 for bothgandf. We observe\nthat  during  the  300-epoch  training  phase  ofgunder  varying  syllabi,  its  loss  approaches","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1257,"to":1275}}}}],["2c510b05-06d2-4486-90e1-e4ec5c39c495",{"pageContent":"Figure 2 presents the losses and success rates on CIFAR10 for bothgandf. We observe\nthat  during  the  300-epoch  training  phase  ofgunder  varying  syllabi,  its  loss  approaches\nzero while its defense success rate rises to above 80%.   This indicates that the proposed\napproach effectively defends against 1-pixel attacks introduced at each validation run. Once\nthe classifier is trained for at least 50 epochs,  it rapidly learns not to be deceived by the\nattacked pixel, resulting in increased success rates forg, while those forfstagnate below\n72%. This finding confirms that the approach generates models that perform on par with the\nundefended model on clean datasets while defending against localized attacks.\n(a)  Training Losses(b)  Training Success Rates\nFigure 2:  CIFAR10 training losses and success rates of defence on EfficientNet.  (a) shows\nthe losses of defended modelgusing different Syllabus configurations and undefended model","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1275,"to":1285}}}}],["c1376daf-9322-44e2-81f9-5a2d3ad0e66f",{"pageContent":"Figure 2:  CIFAR10 training losses and success rates of defence on EfficientNet.  (a) shows\nthe losses of defended modelgusing different Syllabus configurations and undefended model\nf. (b) shows the training success rates of both models under 1-pixel attack.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1285,"to":1287}}}}],["172ceb0f-2477-450f-9b03-8bb7a0e19caf",{"pageContent":"368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n400\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\nAUTHOR(S):9\n5    Experiment and Discussion\nWe evaluate the approach on CNN models trained using popular and contemporary CNN\narchitectures;  EfficientNet-B0  [21],  InceptionV3  [20],  ResNet50  [10]  and  VGG16  [18].\nWe  convert  the  open-source  implementation  of  each  model  to  a  3D-input-compatible  ar-\nchitecture using the modifications highlighted above. All models are implemented using the\nopens-source TensorFlow [1] library. We used FoolBox[17] implementations of N-pixel at-\ntacks.Attack Strategy:We consider N-pixel attacks, whereNis taken from the interval\n[1,2×p\nw\n×p\nh\n]and patches of size up to 25% of the total image area.  During validation,\nthe attack is constrained to 4 regions (Figure??).  If the attacked region size spans multiple\nquadrants, the attack is placed at the center of the image.  The average of the five attacked","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1289,"to":1349}}}}],["21be4427-4515-4199-8f72-15b03d90ef3c",{"pageContent":"the attack is constrained to 4 regions (Figure??).  If the attacked region size spans multiple\nquadrants, the attack is placed at the center of the image.  The average of the five attacked\nclassifications results is then considered the final classification under attack setting.  The at-\ntacked pixels from each quadrant are selected at random. This strategy ensures a focused and\nfair evaluation of our approach and provides a comprehensive assessment of its robustness.\nDatasets:We compared the performance of the original modelfand a defended modelg\non CIFAR10, 100 and ImageNet benchmark datasets under different attack settings. We use\nCIFAR10 to do comprehensive study and CIFAR100 and ImageNet to assess the general-\nizability of the approach.  We compare our results to baseline defense approach that uses\nsimilar datasets.Metrics:We measure the classification accuracy of the models on both\nclean images (acc\nclean\n) and adversarial images (acc\nattack\n). We calculate the robustness score","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1349,"to":1363}}}}],["af848532-aa78-4867-bb62-6e19d07d2ec5",{"pageContent":"similar datasets.Metrics:We measure the classification accuracy of the models on both\nclean images (acc\nclean\n) and adversarial images (acc\nattack\n). We calculate the robustness score\n(δ) as the difference between the model’s classification accuracy on adversarial images and\nits classification accuracy on clean images,δ= (acc\nattack\n−acc\nclean\n).  A largerδdemon-\nstrates greater proactive robustness.  We also measure the defense success rate (β) — the\npercentage of successfully defended attacks. To calculate the defense success rate, we attack\na portion of the images in the test set, while the robustness score is calculated using all im-\nages under attack. A higher defense success rate indicates a better proactive defense both in\nexperimental settings and real-world scenarios.\nDefense effectiveness:  We evaluate the performance of our defense in reducing the ef-\nfectiveness of N-Pixel and APA attacks.  We used 1, 2, up to 16-pixel coverage for N-pixel.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1363,"to":1381}}}}],["4d008b4e-444b-439e-9bc4-a6a45332a2ad",{"pageContent":"Defense effectiveness:  We evaluate the performance of our defense in reducing the ef-\nfectiveness of N-Pixel and APA attacks.  We used 1, 2, up to 16-pixel coverage for N-pixel.\nWe use adversarial patches – Toaster, School-Bus, Lipstick and Pineapple - synthesized by\nattack methods A-ADS [2], covering upto 25% of the entire image.  Our approach is com-\npared  with  existing  defense  strategies  in  terms  of  clean  accuracy  and  attack  accuracy  or\ndefense success rate.   We mount such attacks against our defence (MI, KL, Entropy,  and\nPSNR syllabi), and an undefended model as a control.The patch sizeP(p\nh\n,p\nw\n)of the vo-\nlumization algorithm for all syllabi is set to 16x16 pixels.  We take reported results of all\nbaseline defenses mentioned in section 2 for comparison.\nFigure  3  illustrates  the  overall  robustness  (δ)  of  EfficientNet  3(a)  and  Inception  3(b)\nagainst N-pixel and patch attacks, respectively. The plots highlight the dependence of model","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1381,"to":1395}}}}],["4026d5e5-9ced-45dd-82d1-09d39ce68ce5",{"pageContent":"Figure  3  illustrates  the  overall  robustness  (δ)  of  EfficientNet  3(a)  and  Inception  3(b)\nagainst N-pixel and patch attacks, respectively. The plots highlight the dependence of model\nrobustness on attack size for both defended and undefended models, with the undefended\nmodel  being  40%  less  accurate  at  worst.   Our  defense  is  effective  for  both  architectures\nat all attack magnitudes.   However,  similar to the undefended model,  the performance of\nour method degrades as the size of the attack increases, indicating a shared vulnerability to\nlarger-scale attacks.\nAs depicted in Table 2(top), our proposed defense demonstrates a significant improve-\nment on N-Pixel attack accuracy compared to the undefended models. The undefended mod-\nels exhibit a sharp decline inAcc\nattack\n.  compared toAcc\nclean\n.  Our proposed method (MI)\nnot only achieves highAcc\nclean\nof 96.3% for EfficientNet and 99% for VGG and Inception\nbut also significantly improvesAcc\nattack","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1395,"to":1413}}}}],["280816d6-9416-417b-b5fa-d080889554cb",{"pageContent":"attack\n.  compared toAcc\nclean\n.  Our proposed method (MI)\nnot only achieves highAcc\nclean\nof 96.3% for EfficientNet and 99% for VGG and Inception\nbut also significantly improvesAcc\nattack\nvalues to 91.3% for EfficientNet, 98.6% for VGG,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1413,"to":1422}}}}],["3d94cf9e-fcb1-44cb-b02b-f19dc121d61a",{"pageContent":"414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n10AUTHOR(S):\nMethod\nAcc\nclean\nAcc\nattack\nEffnVGGInceptionEffnVGGInception\nUndefended9898.99944.335.912.3\nMI/Ours96.3999991.398.696.12\nPSD-99.53--97.8-\nLiu et al.-89.8--91-\nMethod\nDefense Success Rate (α)\nCIFAR10CIFAR100ImageNet\nH/Ours (Effn)91.380.5-\nMI/Ours (ResNet)95.695.4389.1\nPSNR/Ours (Inception)96.1294.383.2\nJujutsu (ResNet)86.555.7-\nLGS93.273.7-\nVax-a-Net(VGG)-91.686.8\nDW(VGG)--65.2\nDW(Inception)--66.2\nTable 2:  (Top) Accuracy of models over the set of test images without attacksAcc\nclean\nand\nwith attackAcc\nattack\n,  reported for all CIFAR10 classes.   Reported as top-1 accuracy of 1\npixel attack for the undefended model, the model defended by our method (MI) and other\ncomparable approaches.   All images in the dataset are attacked for this report.   (Bottom)\nDefense success rate (α) of various defense methods against APA covering at least 5% of","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1424,"to":1500}}}}],["fa7217c9-d739-477c-9f48-90153fcd6918",{"pageContent":"comparable approaches.   All images in the dataset are attacked for this report.   (Bottom)\nDefense success rate (α) of various defense methods against APA covering at least 5% of\nthe image. Adversary patches; toaster, lipstick, pineapple, and school-bus were used.\nand 96.12% for Inception. Comparing MI with the PSD method, our approach has a slightly\nlowerAcc\nclean\nfor VGG, with a difference of 0.53%, but delivers a betterAcc\nattack\nfor the\nsame model, with an improvement of 1.2%. When comparing MI to Liu et al.’s method, our\nmethod demonstrates a substantial improvement inAcc\nclean\nfor VGG, with a difference of\n9.2%, and a higherAcc\nattack\nas well, with an improvement of 4.6%.  Notably, PSD and Liu\net al. do not provide results for EfficientNet and Inception.\nTable 2(bottom) presents defense success rates (α) for various defense methods and ours\nagainst APA on three datasets. For CIFAR10, our methods achieved 95.6% and 96.12% suc-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1500,"to":1518}}}}],["46d89046-d4cb-4d38-bc0b-12965d207ec7",{"pageContent":"Table 2(bottom) presents defense success rates (α) for various defense methods and ours\nagainst APA on three datasets. For CIFAR10, our methods achieved 95.6% and 96.12% suc-\ncess rates, while Jujutsu and LGS obtained only 86.5% and 93.2%, respectively.  Similarly,\nfor CIFAR100, our methods reached success rates of 95.43% and 94.3%, outperforming Ju-\njutsu’s 55.7% and LGS’s 73.7%.   In the ImageNet dataset,  MI/Ours achieved the highest\ndefense success rate of 89.1%, while PSNR/Ours obtained 83.2%, both surpassing Vax-a-\nNet’s 86.8% and DW’s 65.2% and 66.2% for VGG and Inception models, respectively. Not\nall methods have reported results for every dataset, limiting a comprehensive comparison of\ntheir effectiveness.\nNetwork Architecture and attack size: Figures 3(c) and 3(d) depict the impact of attack\nsize on various CNN architectures, revealing that the defended models are highly resilient\n– compared to the undefended model – as the attack magnitude increases.  However, once","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1518,"to":1529}}}}],["a0148e0e-5e16-4bec-9a32-ecb2d99343c7",{"pageContent":"size on various CNN architectures, revealing that the defended models are highly resilient\n– compared to the undefended model – as the attack magnitude increases.  However, once\nthe attacked pixels surpass the individual patch size of the volume, the defense effectiveness\nbegins to wane.  This behavior can be attributed to the fact that the volumization algorithm\ncounters small size attacks.  When the perturbation size exceeds the patch size or if it spans\nmore  than  one  patch  in  the  volume,  the  algorithm’s  ability  to  isolate  and  counteract  the\nadversarial noise diminishes.\nTiming information: Table 3(e) compares the time taken for inference using our method\nand undefended. An inference pass on the defended model takes about 6 milliseconds more\ntime compared to the undefended.  The overhead is primarily due to the volumization al-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1529,"to":1538}}}}],["c7546666-0ce2-4918-9493-a530a82b0488",{"pageContent":"460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n500\n501\n502\n503\n504\n505\nAUTHOR(S):11\ngorithm applied at preprocessing stage.  However our defence does incur significant more\noverhead during training.  It can incur hours to days depending on the size of the dataset.\nThis process only needs to be run once, as does training the model a priori.  All inference\nruns used NVIDIA RTX A4000 GPU and training was done on a multi-gpu cluster.\n(a)  EfficientNet(b)  Inception\n(c)  N-Pixel(d)  APA\nMethodVGGResNetInception\nUndefended0.120.140.09\nMI/Ours0.230.180.16\n(e)  Inference Time\nFigure 3:  (a) EfficientNet robustness as a function of N - number of pixels attacked,  (b)\nInception robustness against APA as a function of patch size, (c) Defense success rate (β)\nof various models as a function of N-Pixel attack magnitude, (d) Defense success rate (β) of\nvarious models as a function of APA attack magnitude, (e) Inference time for VGG, ResNet,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1540,"to":1600}}}}],["a914af26-a6f7-42b0-81e3-03038e22eb2b",{"pageContent":"of various models as a function of N-Pixel attack magnitude, (d) Defense success rate (β) of\nvarious models as a function of APA attack magnitude, (e) Inference time for VGG, ResNet,\nand Inception, (f) Training time vs Epoch for an undefended model trained on ImageNet and\nthe same model with our defense based on MI syllabus.\n6    Conclusion\nWe introduced a proactive defense approach against localized adversarial attacks, which pre-\nserves model performance on clean data.  Our method combines a volumization algorithm\nthat converts 2D images into 3D volumetric representations while maintaining spatial rela-\ntionships, increasing resilience to perturbations. Additionally, we employ a deep curriculum\nlearning optimization strategy, ordering training samples by complexity, enabling progres-\nsive learning from simple to complex samples. By incorporating these techniques into popu-\nlar CNN architectures, we demonstrated the effectiveness of our method against N-pixel and","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1600,"to":1611}}}}],["fa963d6e-447a-4ecf-82e0-10bbca1b88ee",{"pageContent":"sive learning from simple to complex samples. By incorporating these techniques into popu-\nlar CNN architectures, we demonstrated the effectiveness of our method against N-pixel and\npatch attacks.  Experimental results indicated improved robustness without sacrificing per-\nformance on clean data, confirming our approach’s ability to enhance image classification\nmodel resilience against localized adversarial attacks.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1611,"to":1615}}}}],["95c9f448-34e3-445e-a6c9-cf0174ce5148",{"pageContent":"506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n12AUTHOR(S):\nReferences\n[1]  Martín Abadi, Paul Barham, Jianmin Chen, Zhifeng Chen, Andy Davis, Jeffrey Dean,\nMatthieu Devin, Sanjay Ghemawat, Geoffrey Irving, Michael Isard, Manjunath Kudlur,\nJosh Levenberg, Rajat Monga, Sherry Moore, Derek Gordon Murray, Benoit Steiner,\nPaul Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Yuan Yu, and Xiaoqiang\nZheng. Tensorflow: Large-scale machine learning on heterogeneous systems.https:\n//tensorflow.org, 2015.\n[2]  Tom B Brown, Dandelion Man’e, Aurko Roy, Mart’in Abadi, and Justin Gilmer.  Ad-\nversarial  patch.arXiv preprint arXiv:1712.09665,  2018.   URLhttp://arxiv.\norg/abs/1712.09665.\n[3]  Anirban  Chakraborty,  M  Alam,  Vikram  Dey,  Ansuman  Chattopadhyay,  and  Deb-\ndeep  Mukhopadhyay.   Adversarial  attacks  and  defences:  A  survey.arXiv preprint\narXiv:1810.00069, 2018. URLhttp://arxiv.org/abs/1810.00069.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1617,"to":1676}}}}],["138df212-d190-417a-8257-92e42ed9c4b7",{"pageContent":"deep  Mukhopadhyay.   Adversarial  attacks  and  defences:  A  survey.arXiv preprint\narXiv:1810.00069, 2018. URLhttp://arxiv.org/abs/1810.00069.\n[4]  Dongxian Chen, Ruiqi Xu, and Bo Han.   Patch selection denoiser:  An effective ap-\nproach defending against one-pixel attacks.  InNeural Information Processing, pages\n286–296. Springer, 2019. doi: 10.1007/978-3-030-36802-9_31.\n[5]  Pin-Yu   Chen,   Yash   Sharma,   Huan   Zhang,   Jinfeng   Yi,   and   Cho-Jui   Hsieh.\nHopskipjumpattack:A   query-efficient   decision-based   attack.arXiv preprint\narXiv:1904.02144, 2019.\n[6]  Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.  Imagenet:  A\nlarge-scale hierarchical image database.2009 IEEE Conference on Computer Vision\nand Pattern Recognition, pages 248–255, 2009. doi: 10.1109/CVPR.2009.5206848.\n[7]  Hailay Ghebrechristos and Gita Alaghband.   Deep curriculum learning optimization.\nSN Computer Science, 1(5):245, 2020. doi: 10.1007/s42979-020-00251-7.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1676,"to":1688}}}}],["b555517e-2cfd-4873-ac14-da9a77e605f6",{"pageContent":"[7]  Hailay Ghebrechristos and Gita Alaghband.   Deep curriculum learning optimization.\nSN Computer Science, 1(5):245, 2020. doi: 10.1007/s42979-020-00251-7.\n[8]  Thomas  Gittings,  Stephen  Schneider,  and  John  Collomosse.    Vax-a-net:   Training-\ntime  defence  against  adversarial  patch  attacks.   In  Hideo  Ishikawa,  Cheng-Lin  Liu,\nTomas Pajdla, and Jianbo Shi, editors,Computer Vision–ACCV 2020, volume 12625,\npages  235–251,  Cham,  2021.  Springer  International  Publishing.doi:    10.1007/\n978-3-030-69538-5_15.\n[9]  Jamie  Hayes.On  visible  adversarial  perturbationsdigital  watermarking.In\n2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops\n(CVPRW), pages 1678–16787, Salt Lake City, UT, USA, 2018. doi: 10.1109/CVPRW.\n2018.00210.\n[10]  Kaiming He,  Xiangyu Zhang,  Shaoqing Ren,  and Jian Sun.   Deep residual learning\nfor image recognition.  InProceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1688,"to":1701}}}}],["18144f28-df2f-4709-bb69-0646fb03ff89",{"pageContent":"for image recognition.  InProceedings of the IEEE conference on computer vision and\npattern recognition, pages 770–778, 2016.\n[11]  Alex    Krizhevsky.Learning    multiple    layers    of    features    from    tiny    im-\nages,2009.URLhttps://www.cs.toronto.edu/~kriz/\nlearning-features-2009-TR.pdf. p. 60.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1701,"to":1705}}}}],["1b8b63c4-e0f4-4631-a7de-889a2cff152e",{"pageContent":"552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\nAUTHOR(S):13\n[12]  Wei Liu, Jiajia Jiang, Yajie Zhang, and Yi Yang.  Deep learning for generic watermark\nremoval.   InProceedings of the 2018 ACM on Multimedia Conference, pages 1336–\n1344. ACM, 2018.\n[13]  Zhi-Yi  Liu,  Po-Shen  Wang,  Shih-Chieh  Hsiao,  and  Ricky  Tso.   Defense  against  n-\npixel attacks based on image reconstruction.  InProceedings of the 8th International\nWorkshop on Security in Blockchain and Cloud Computing, pages 3–7, New York, NY,\nUSA, 2020. doi: 10.1145/3384942.3406867.\n[14]  Jan  Hendrik  Metzen,  Tim  Genewein,  and  Volker  Fischer.   Detecting  and  defending\nagainst adversarial attacks on neural networks.  InInternational Conference on Learn-\ning Representations, 2017.\n[15]  Muzammal Naseer, Salman Khan, and Fatih Porikli.  Local gradients smoothing:  De-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1707,"to":1764}}}}],["0c5658fb-bbd3-41f8-9b30-800ab5a52e44",{"pageContent":"against adversarial attacks on neural networks.  InInternational Conference on Learn-\ning Representations, 2017.\n[15]  Muzammal Naseer, Salman Khan, and Fatih Porikli.  Local gradients smoothing:  De-\nfense against localized adversarial attacks.  In2019 IEEE Winter Conference on Ap-\nplications of Computer Vision (WACV), pages 1300–1307, 2019. doi: 10.1109/WACV.\n2019.00143.\n[16]  Srijan  Rao,  David  Stutz,  and  Bernt  Schiele.   Adversarial  training  against  location-\noptimized  adversarial  patches.    In  Hideo  Ishikawa,  Cheng-Lin  Liu,  Tomas  Pajdla,\nand Jianbo Shi, editors,Computer Vision–ACCV 2020, volume 12539, pages 429–448,\nCham, 2020. Springer International Publishing. doi: 10.1007/978-3-030-68238-5_32.\n[17]  Jonas Rauber, Wieland Brendel, and Matthias Bethge.  Foolbox:  A python toolbox to\nbenchmark the robustness of machine learning models.https://github.com/\nbethgelab/foolbox, 2017.\n[18]  Karen Simonyan and Andrew Zisserman.  Very deep convolutional networks for large-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1764,"to":1777}}}}],["9abbee71-1770-4a93-ae10-ecc9188567b2",{"pageContent":"benchmark the robustness of machine learning models.https://github.com/\nbethgelab/foolbox, 2017.\n[18]  Karen Simonyan and Andrew Zisserman.  Very deep convolutional networks for large-\nscale image recognition.   InInternational Conference on Learning Representations,\n2015.\n[19]  Jiawei  Su,  Danilo  Vasconcellos  Vargas,  and  Sakurai  Kouichi.   One  pixel  attack  for\nfooling deep neural networks.   2017.   URLhttps://arxiv.org/abs/1710.\n08864.\n[20]  Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna.\nRethinking the inception architecture for computer vision. InProceedings of the IEEE\nconference on computer vision and pattern recognition, pages 2818–2826, 2016.\n[21]  Mingxing Tan and Quoc V. Le.  Efficientnet:  Rethinking model scaling for convolu-\ntional neural networks.  InProceedings of the 36th International Conference on Ma-\nchine Learning, pages 6105–6114. PMLR, 2019.\n[22]  Chaowei Xiang and Prateek Mittal. Detectorguard: Provably securing object detectors","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1777,"to":1791}}}}],["97480978-9984-450d-b010-b1a7f53dac5c",{"pageContent":"chine Learning, pages 6105–6114. PMLR, 2019.\n[22]  Chaowei Xiang and Prateek Mittal. Detectorguard: Provably securing object detectors\nagainst localized patch hiding attacks.arXiv preprint arXiv:2102.02956, 2021.  doi:\n10.48550/arXiv.2102.02956.\n[23]  Yingyu  Zhang,  Yang  Song,  Hairong  Qi,  Yingjie  Xia,  Cho-Jui  Hsieh,  and  Le  Song.\nDetecting adversarial attacks via data distribution discrepancy.  InInternational Con-\nference on Learning Representations, 2019.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/POR_BMVC23_draft_2.pdf","pdf_numpages":13,"loc":{"lines":{"from":1791,"to":1797}}}}],["a12241b9-2b14-4223-bcc6-5081274fe410",{"pageContent":"Citation:McCarthy, Shawn, and Gita\nAlaghband. 2023. Enhancing\nFinancial Market Analysis and\nPrediction with Emotion Corpora\nand News Co-Occurrence Network.\nJournal of Risk and Financial\nManagement16:  226.  https://\ndoi.org/10.3390/jrfm16040226\nAcademic Editor: David Liu\nReceived: 20 February 2023\nRevised: 23 March 2023\nAccepted: 26 March 2023\nPublished: 4 April 2023\nCopyright:©  2023  by  the  authors.\nLicensee   MDPI,   Basel,   Switzerland.\nThis  article  is  an  open  access  article\ndistributed    under   the    terms   and\nconditions of the Creative Commons\nAttribution (CC BY) license (https://\ncreativecommons.org/licenses/by/\n4.0/).\nJournal of\nRisk and Financial\nManagement\nArticle\nEnhancing Financial Market Analysis and Prediction with\nEmotion Corpora and News Co-Occurrence Network\nShawn McCarthy *\nand Gita Alaghband\nDepartment of Computer Science and Engineering, University of Colorado Denver, Denver, CO 80204, USA\n*Correspondence: shawn.mccarthy@ucdenver.edu; Tel.: +1-(303)-349-9745\nAbstract:","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1,"to":32}}}}],["730c5e3d-6258-4320-9d90-c343a6182248",{"pageContent":"and Gita Alaghband\nDepartment of Computer Science and Engineering, University of Colorado Denver, Denver, CO 80204, USA\n*Correspondence: shawn.mccarthy@ucdenver.edu; Tel.: +1-(303)-349-9745\nAbstract:\nThis study employs an improved natural language processing algorithm to analyze over\n500,000 financial news articles from sixteen major sources across 12 sectors, with the top 10 companies\nin each sector. The analysis identifies shifting economic activity based on emotional news sentiment\nand develops a news co-occurrence network to show relationships between companies even across\nsectors. This study created an improved corpus and algorithm to identify emotions in financial news.\nThe improved method identified 18 additional emotions beyond what was previously analyzed. The\nresearchers labeled financial terms from Investopedia to validate the categorization performance of\nthe new method. Using the improved algorithm, we analyzed how emotions in financial news relate","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":32,"to":43}}}}],["c86fa5a4-ca4b-4414-a27a-8c624cab8f01",{"pageContent":"researchers labeled financial terms from Investopedia to validate the categorization performance of\nthe new method. Using the improved algorithm, we analyzed how emotions in financial news relate\nto market movement of pairs of companies. We found a moderate correlation (above 60%) between\nemotion sentiment and market movement. To validate this finding, we further checked the correlation\ncoefficients between sentiment alone, and found that consumer discretionary, consumer staples,\nfinancials, industrials, and technology sectors showed similar trends.  Our findings suggest that\nemotional sentiment analysis provide valuable insights for financial market analysis and prediction.\nThe technical analysis framework developed in this study can be integrated into a larger investment\nstrategy, enabling organizations to identify potential opportunities and develop informed strategies.\nThe insights derived from the co-occurrence model may be leveraged by companies to strengthen","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":43,"to":52}}}}],["74b76c54-12a6-4afb-a410-71d24faa17b2",{"pageContent":"strategy, enabling organizations to identify potential opportunities and develop informed strategies.\nThe insights derived from the co-occurrence model may be leveraged by companies to strengthen\ntheir risk management functions, making it an asset within a comprehensive investment strategy.\nKeywords:NLP; emotional sentiment analysis; financial news; co-occurrence graph\n1. Introduction\nFinancial  market  analysis  covers  a  wide  range  of  topics,  including  asset  pricing,\nmarket efficiency, and financial market anomalies. One important aspect of this field is risk\nmanagement, which focuses on addressing uncertainties arising from financial markets; one\naspect of that uncertainty is market sentiment impact (Shapiro et al. 2020). Karen Horcher\ndefines financial risk management as the process of identifying, assessing, and controlling\nfinancial risks that may impact an organization’s ability to achieve its financial objectives.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":52,"to":62}}}}],["c59e36fb-23a3-42cc-a550-6c6508e7ca19",{"pageContent":"defines financial risk management as the process of identifying, assessing, and controlling\nfinancial risks that may impact an organization’s ability to achieve its financial objectives.\nThis includes identifying risks related to credit, market, liquidity, operational, and other\nareas, and developing strategies and techniques to mitigate those risks (Horcher 2011).\nThe resulting emotional analysis of financial news and co-occurrence network promise\nto provide another dimension to help in financial market analysis and enable investors\nto gain a better understanding of how the market is likely to react.   By analyzing the\nemotional content of news articles, organizations can gain insights into potential risks and\nopportunities and develop strategies to mitigate or capitalize on them.\nChanges in key economic indicators have historically provided a reliable guide to\nrecognizing the business cycle’s four distinct phases—early, mid, late, and recession. Our","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":62,"to":72}}}}],["20e0fa65-8b0f-4516-a2e6-fbdc063510f0",{"pageContent":"Changes in key economic indicators have historically provided a reliable guide to\nrecognizing the business cycle’s four distinct phases—early, mid, late, and recession. Our\napproach seeks to identify the shifting market movements within a sector, providing a\nframework for making asset allocation decisions according to the probability that assets\nmay outperform or underperform based on the emotional factors of financial news articles.\nJ. Risk Financial Manag.2023,16, 226. https://doi.org/10.3390/jrfm16040226https://www.mdpi.com/journal/jrfm","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":72,"to":77}}}}],["78a3984c-4f7e-4ad2-9d88-e63038f7e475",{"pageContent":"J. Risk Financial Manag.2023,16, 2262 of 19\nThis approach may be incorporated into an asset allocation framework to take advan-\ntage of financial news media impact on performance that may deviate from longer-term\nasset returns.\nEconomic researchers (Ibbotson and Kaplan 2000) state that economic factors influence\nasset prices; however, there is still research required to determine the best way to incorpo-\nrate additional factors such as the emotional impact of financial news into asset allocation\napproaches. The impact of the pandemic entered the US into a contraction after peak cycle\nthat was not influenced by other economic factors.  We believe that with a disciplined\napproach we can better predict the emotional news correlation to sector volatility and better\nanalyze the underlying factors and trends across various time horizons using both news\nsources and sector data sources.\nIn this paper, we present a body of research across two stages. In phase 1, we collect","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":79,"to":91}}}}],["221c9c89-97e8-49f6-9e0c-3c3c1af804d4",{"pageContent":"sources and sector data sources.\nIn this paper, we present a body of research across two stages. In phase 1, we collect\na large set of financial news articles leveraging named entity recognition (concepts) to\nidentify news articles with the specific companies representing the top 10 across each of the\nmain 12 sectors by holdings resulting in 516,973 news articles to analyze for the period from\n1 January 2019 to 1 January 2021. We modify the text2emotion library (Gupta et al. 2021) to\ninclude a larger corpus by incorporating the National Research Council Lexicon financial\nglossary and modify to expand from 5 emotions (text2emotion: happy, angry, sad, surprise,\nand fear) to include the 8 main emotions (NRC lexicon:  anger, fear, anticipation, trust,\nsurprise, sadness, joy, and disgust) and incorporate emotional mixing with 22 emotions\nproviding categorization of 30 distinct emotions.\nIn  phase  2,  we  conduct  sector  analysis  over  the  same  time-period.   We  calculate","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":91,"to":102}}}}],["17f88840-aff6-44bb-96e3-45305643f6c7",{"pageContent":"providing categorization of 30 distinct emotions.\nIn  phase  2,  we  conduct  sector  analysis  over  the  same  time-period.   We  calculate\npercent change in sector price as compared to the previous day to identify more moderate\nmarket movement events (1% is noted as a normal) as noted by Ed Easterling, Crestmont\nResearch, “The average daily swing over more than forty years has been approximately\n1.4%.” (Easterling 2022). We leverage this daily percent change across each sector to look\nfor significant events that we define any market movements±2% across 1 January 2019\nthrough 1 January 2021, which brings total articles processed down to 111,175. We then\nleverage the new emotion classification algorithm with named entity recognition based\non unique Wikipedia concepts (e.g., http://en.wikipedia.org/wiki/COMPANYNAME\n(accessed on 24 April 2022) Table A1) to identify specific companies using news articles to\ncreate a co-occurrence network based on the companies appearing in the news, in the same","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":102,"to":113}}}}],["3450d0ad-bd1f-48a5-b09f-6c546f7405b0",{"pageContent":"(accessed on 24 April 2022) Table A1) to identify specific companies using news articles to\ncreate a co-occurrence network based on the companies appearing in the news, in the same\nday, with the same emotional classification of the articles.\nSentiment polarity analysis has been leveraged in recent research for predicting stock\nmarket prices (Lu et al. 2021), correlation with financial news (Wan et al. 2021), as well as\nrecent news co-occurrence approaches (Tang et al. 2019) finding a statistically significant\nassociation between media sentiment and abnormal market return.   We continue this\nresearch and expand it to look deeper into the emotional sentiment as a stronger correlation\napproach than polarity can provide on its own. Although there are many approaches to\nleveraging emotional analysis of tweets, there has not been research on applying them\nto  financial  news  (Aslam  et  al.  2022;  Ramírez-Sáyago  2020).   The  popular  tool  in  the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":113,"to":123}}}}],["daa85e1f-d581-42a8-bef6-851f398ccd0f",{"pageContent":"leveraging emotional analysis of tweets, there has not been research on applying them\nto  financial  news  (Aslam  et  al.  2022;  Ramírez-Sáyago  2020).   The  popular  tool  in  the\nresearcher’s toolkit is Text2Emotion.   However,  we require a much larger corpus that\nincorporates financial terms and the leading NRC Emotion Lexicons from the National\nResearch Council to create a more robust corpus for this analysis that leverages the eight\nemotions from Plutchik’s research.  Plutchik suggests that people experience eight core\nor primary emotions (joy and sadness, trust and loathing, fear and anger, surprise, and\nanticipation). The eight primary emotions can then be combined into twenty-four primary,\nsecondary, and tertiary dyads defined as feelings composed of two emotions representing\na significant amount of overall emotion.  Further aligning to this more recent emotional\nresearch  around  Plutchik’s  emotional  wheel  (original  basis  of  the  NRC  lexicons)  and","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":123,"to":133}}}}],["eed62c54-a778-45e2-9193-de92ba181f77",{"pageContent":"a significant amount of overall emotion.  Further aligning to this more recent emotional\nresearch  around  Plutchik’s  emotional  wheel  (original  basis  of  the  NRC  lexicons)  and\nincorporating what these lexicons ignore, i.e., the three dyads of emotion that look at the\nmixture of prevalent emotions (e.g., joy + trust indicative of the feeling of love), adding\n24 additional emotional classifications to the annotation algorithm.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":133,"to":137}}}}],["96992f6e-cbfd-4e67-a088-12bafc7645c6",{"pageContent":"J. Risk Financial Manag.2023,16, 2263 of 19\nWe agree with Wan in the articulation of the complexity of financial analysis across\nbusiness cycles, macroeconomics, and sectors. “Complexity and inter-dependencies have\nbeen the defining features of most modern financial markets: a myriad of ever-changing in-\nteractions between market participants, financial assets and relations with broader macroe-\nconomic factors have all contributed to intricate market dynamics.” (Wan et al. 2021). We\nfurther expand this research to allow us to study the dynamics of emotion and expand the\ntarget companies across a larger set of sectors that better represent the significant 12 sectors\nof the economy and relate those to more significant market corrections of 2%. Our hypothe-\nsis is “there is a stronger correlation between companies that share the same emotion on the\nsame day representing a stronger correlation than simple sentiment polarity can provide.”.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":139,"to":149}}}}],["3ae3ee80-7ca7-4e82-b784-2bddd1b55157",{"pageContent":"sis is “there is a stronger correlation between companies that share the same emotion on the\nsame day representing a stronger correlation than simple sentiment polarity can provide.”.\nThe primary contributions of this paper are (1) the introduction of an improved lan-\nguage corpus (19,430 terms compared to original 8666 terms), (2) financial phrases with the\nability to incorporate multi-word phrases, and (3) expanding from 5 to an expanded 30 emo-\ntions.  In the labeling of Investopedia terms and phrases dictionary, we compared how\nthe article would have been interpreted by Text2Emotion Corpus with the addition of the\nNRC dataset bringing the understood vocabulary from 8666 to 13,470 with newly aligned\nPlunkett emotions and mixed emotions finding improved categorization of the emotion.\nThe secondary contribution of this paper is an efficient way to create co-occurrence\nnews networks based on the improved emotional library where we find moderate correla-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":149,"to":159}}}}],["5b2dad8f-0641-4c61-a9e2-79abb12ba39c",{"pageContent":"The secondary contribution of this paper is an efficient way to create co-occurrence\nnews networks based on the improved emotional library where we find moderate correla-\ntion between sectors based on emotion over sentiment polarity alone (created with compa-\nnies in those sectors that show up in the news in the same day with the same emotion).\nThis paper is organized around Section 2: related work, Section 3: methods and mate-\nrials for emotional annotation algorithm improvements and development of the financial\nnews co-occurrence network, concluding in Section 4 with notable results in the emo-\ntional annotation algorithm improvements and findings related to relationships between\ncompanies and their sectors based on the emotional annotation of financial news articles.\n2. Related Work\nSentiment analysis as a research area has been an active and important field most\nhighly attributed to the use of social platforms. Microsoft, as part of their communication","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":159,"to":170}}}}],["7cbd7c68-d90c-4e7f-ac66-b1b5abc58af8",{"pageContent":"2. Related Work\nSentiment analysis as a research area has been an active and important field most\nhighly attributed to the use of social platforms. Microsoft, as part of their communication\ncompliance platform, leverages machine learning to detect different types of emotion such\nas harassment to minimize communication risk by helping companies detect, capture and\nact on messages deemed inappropriate (Mazzolli et al. 2023).\nIn “Tweet Emotion Dynamics: Emotion Word Usage in Tweets from US and Canada”\n(Vishnubhotla and Mohammad 2022), the authors look at twitter data as one of the most\ninfluential forums for social, political, and health discourse. Developing a (TED) metric\nto capture patterns of emotion associated with tweets over time, the authors leverage\nthe NRC lexicon (valence, arousal, and dominance) to determine emotion associations.\nThese are numerical scores, where a valence score of≥0.67 represents positive or polar","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":170,"to":181}}}}],["e9f4e1dc-21b0-4aae-a5e1-bea0c52aa9b6",{"pageContent":"the NRC lexicon (valence, arousal, and dominance) to determine emotion associations.\nThese are numerical scores, where a valence score of≥0.67 represents positive or polar\nterms to determine the emotion association of the words in tweets.  The authors note\nthat similar analysis could be carried out using the NRC emotion Lexicon to perform\ncategorical and dimensional analysis of emotions. We take this inspiration to leverage the\nNRC emotion lexicon and further expand emotions from five to eight and incorporating\nthe larger expanded Plutchik’s research representing 30 distinct emotions and including a\nfinancial glossary to support financial news research.\nIn “Emotion in Twitter communication and stock prices of firms: the impact of Covid-\n19 pandemic” (Dhar and Bose 2020), the authors leverage TextBlob and Text2Emotion to\nleverage polarity (positive and negative) and emotional analysis from the Text2Emotion\nembedded lexicon. We expand on this paper to include financial terminology and multi-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":181,"to":192}}}}],["4c4f9d74-ffd4-4232-a9ab-00fabedffef9",{"pageContent":"leverage polarity (positive and negative) and emotional analysis from the Text2Emotion\nembedded lexicon. We expand on this paper to include financial terminology and multi-\nword identification to better analyze financial articles. Our approach looks to leverage this\nlarger corpus to then scrape all terminology from Investopedia financial terms (6253) to\ncreate a financial lexicon to incorporate into the corpus.  As financial phrases tend to be\nmulti-word, we also needed to add lookahead logic to look for financial phrases in the\ncalculation of the emotional category.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":192,"to":198}}}}],["81e691ad-8268-4809-bc84-3f18761553c1",{"pageContent":"J. Risk Financial Manag.2023,16, 2264 of 19\nIn “Sentiment correlation in financial news networks and associated market move-\nments” (Wan et al. 2021), authors Wan et al.  leverage sentiment analysis and new co-\noccurrence (companies appearing in news) to build the graph model used in the analysis.\nWe expand on this idea to leverage our new emotional algorithm so that companies that\nappear on the news on the same day with the same emotion whose sector had a more\nsignificant market correction (±2%). Weights are added to the edges based on the number\nof unique emotions are shared across different days (two companies share joy on day 1\nand anger on day 2) representing a much stronger correlation between those edges. This\nresults in an efficient way to create co-occurrence news networks based on the improved\nemotional categorization that polarity alone is unable to do.\n3. Materials and Methods\nIn this section, we briefly describe the workflow commonly used by analysts when","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":200,"to":212}}}}],["bef7ab9a-59b8-4ba8-8a3b-7739dede7750",{"pageContent":"emotional categorization that polarity alone is unable to do.\n3. Materials and Methods\nIn this section, we briefly describe the workflow commonly used by analysts when\nconducting  risk  assessment  and  financial  analysis  leveraging  time  series  information.\nResearchers have focused on applying time series analysis to market performance. When\nsignificant negative events occur that could impact a company’s operations, the stock\nprice tends to decrease. Essentially, the stock price serves as a barometer of the market’s\nconfidence in a company’s future performance.  Time series analysis can be used in risk\nmanagement to identify, model, and forecast changes in financial market variables over time.\nThis includes modeling the volatility of financial returns, predicting future market trends\nand movements, and identifying potential risk factors that may impact financial outcomes\n(El-Qadi et al. 2022; Huang 2016).  Our methodology is based on adding additional risk","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":212,"to":223}}}}],["adb223c2-20f4-42e1-a353-6b93da97d22f",{"pageContent":"and movements, and identifying potential risk factors that may impact financial outcomes\n(El-Qadi et al. 2022; Huang 2016).  Our methodology is based on adding additional risk\nfactors, which is the news emotion sentiment, to create a co-occurrence news network\nof companies that share emotional sentiment on the same day to improve accuracy.  In\nthis research, we used a natural language processing library we improved to leverage\nthe EmoLex lexicon (https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm\n(accessed on 24 April 2022)),  domain-specific phrases,  and mixed emotion analysis to\nimprove the categorization of news articles, providing a more accurate in-day correlation\nbetween companies. Substantial materials were reviewed including macroeconomic, sector\ndata, and a very large, queried news dataset tied to the companies in the sectors being\nanalyzed to serve as the basis to create graph of companies that share emotional sentiment.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":223,"to":233}}}}],["10a2bce3-0bcc-4db4-870e-b6ec33ee7971",{"pageContent":"data, and a very large, queried news dataset tied to the companies in the sectors being\nanalyzed to serve as the basis to create graph of companies that share emotional sentiment.\nWe first briefly outline our steps of data collections and our algorithms developed\nto analyze time series sector data, and the correlation to emotional sentiment in financial\nnews articles. We then provide a more detailed description in Sections 3.1 and 3.2.\n1. Data collection: The first step was to collect the necessary data, including financial\nmarket data for the sector of interest and financial news articles related to that sector. The\ndata were obtained from the federal reserve and economic data (FRED) for macroeco-\nnomic data and from finance.yahoo.com for sector-related information, financial news\nwas obtained across several financial news sources, standard news sources, and inter-\nnational (“nasdaq.com”, “barrons.com”, “thestreet.com”, “investing.com”, “forbes.com”,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":233,"to":243}}}}],["7be564ed-85cf-4589-926d-0def0e98d7ce",{"pageContent":"was obtained across several financial news sources, standard news sources, and inter-\nnational (“nasdaq.com”, “barrons.com”, “thestreet.com”, “investing.com”, “forbes.com”,\n“wash-ingtonpost.com”, “nytimes.com”, “reuters.com”, “foxnews.com”, “bloomberg.com”,\n“ya-hoo.com”, “cnn.com”, “wsj.com”, “cnbc.com”, “marketwatch.com”, “bbc.com”).\n2.  Data cleaning and preprocessing: The next step was to clean and preprocess the\ndata using pandas, a Python data analysis library.  This involved removing duplicates,\nmissing values, and irrelevant data. The data were then transformed into a format suitable\nfor analysis, such as a time series dataset and percentage of change from one day to next.\n3. Sentiment analysis: To analyze the emotional sentiment of the financial news articles,\nsentiment analysis algorithms were used. These algorithms use natural language processing\ntechniques to identify and extract sentiment-related information and emotional information","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":243,"to":253}}}}],["04b144b3-568e-4660-8bde-99325d460a38",{"pageContent":"sentiment analysis algorithms were used. These algorithms use natural language processing\ntechniques to identify and extract sentiment-related information and emotional information\nfrom the text.  We leveraged the Natural Language Toolkit (NLTK) SentimentAnalyzer\n(Bird et al. 2009) for sentiment score and modified the Text2Emotion (Gupta et al. 2021) to\ninclude the EmoLex lexicon and expanded finance vocabulary using SentimentAnalyzer to\nadjust for sentence content in the calculation of emotion.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":253,"to":258}}}}],["9de5accf-9cc0-4b6c-8f17-c4c0babc5828",{"pageContent":"J. Risk Financial Manag.2023,16, 2265 of 19\n4. Correlation analysis: Once the emotions were obtained for each article, the next step\nas to correlate them with the time series sector data for the financial sector and company\ndata. This can was performed by using pandas and statistical analysis techniques to create\na co-occurrence graph based on the same emotion for companies in the news with the\nemotion categorization algorithm. This graph, based on number of articles with the same\nemotion, creates greater strengths between the companies and highlights notable pairs of\ncompanies.  We used correlation coefficients on sector sentiment to see how closely the\nsectors matched up with the movement of the companies and found similar trends.\n5. Visualization and interpretation: Finally, the results were visualized and interpreted\nto gain insights into the relationship between emotional sentiment in financial news articles","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":260,"to":270}}}}],["36c36f1f-82d6-47d2-9b13-4108cd6c5bc3",{"pageContent":"5. Visualization and interpretation: Finally, the results were visualized and interpreted\nto gain insights into the relationship between emotional sentiment in financial news articles\nand the behavior of the financial market. This can help investors make informed decisions\nand predict market trends.\nFirst,  we presented the improved emotional annotation algorithm expanding the\nvocabulary, expanding categorization, and incorporating a financial glossary (domain-\nspecific corpus). We then covered the development of the news co-occurrence network by\ndownloading financial news and market financial data and analyzing those articles across\nmarket events categorized by the emotional annotation algorithm. We used this approach to\nfind relationships between companies and their sectors based on daily emotional annotation\noccurring during market events.\n3.1. Emotional Annotation Algorithm\nTo be able to analyze the financial news, we needed an algorithm that included finan-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":270,"to":282}}}}],["ace98a91-aae8-4a9c-bc56-0a1f9e63a0fa",{"pageContent":"occurring during market events.\n3.1. Emotional Annotation Algorithm\nTo be able to analyze the financial news, we needed an algorithm that included finan-\ncial terms, an improved language corpus with additional words, the ability to incorporate\nmulti-word phrases, and an expanded emotional dialect (30 emotions).  The original al-\ngorithm had five primary emotions, whereas the NRC dataset included eight based on\nPlutchik’s model, requiring us to normalize the Text2Emotion embedded corpus. We also\nexpanded to include the addition of 22 mixed emotions and improvements to leverage\nsentiment to emphasize the calculation. Logical Model 1 shows the high-level improve-\nments to the original Text2Emotion algorithm from 5 to 8 emotions, expanded corpus,\nand sentiment improvements. Logical Model 2 shows leveraging the new logical model\n1 to annotate the domain specific knowledge to then expand domain specific knowledge\ninto the corpus to include phrases and further expanding emotion annotation with the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":282,"to":294}}}}],["74c934e9-05ac-451b-a7ae-9ba2e76d3588",{"pageContent":"1 to annotate the domain specific knowledge to then expand domain specific knowledge\ninto the corpus to include phrases and further expanding emotion annotation with the\nremaining 22 emotions.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 5 of 19 \n \n \nprocessing  techniques  to  identify  and  extract  sentiment-related  information  and  emo-\ntional information from the text. We leveraged the Natural Language Toolkit (NLTK) Sen-\ntimentAnalyzer  (Bird et  al. 2009)  for  sentiment  score  and  modified the  Text2Emotion \n(Gupta et al. 2021) to include the EmoLex lexicon and expanded finance vocabulary using \nSentimentAnalyzer to adjust for sentence content in the calculation of emotion. \n4. Correlation  analysis:  Once  the  emotions were obtained  for  each  article,  the  next \nstep as to correlate them with the time series sector data for the financial sector and com-\npany data. This can was performed by using pandas and statistical analysis techniques to","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":294,"to":307}}}}],["28fdb179-7006-4468-89eb-e0e6e3f330aa",{"pageContent":"step as to correlate them with the time series sector data for the financial sector and com-\npany data. This can was performed by using pandas and statistical analysis techniques to \ncreate a co-occurrence graph based on the same emotion for companies in the news with \nthe emotion  categorization  algorithm.  This  graph, based  on  number  of articles with  the \nsame  emotion, creates greater  strengths  between  the  companies  and  highlights notable \npairs of companies. We used correlation coefficients on sector sentiment to see how closely \nthe sectors matched up with the movement of the companies and found similar trends. \n5. Visualization  and  interpretation:  Finally,  the  results were visualized  and  inter-\npreted  to  gain  insights  into  the  relationship  between  emotional  sentiment  in  financial \nnews articles and the behavior of the financial market. This can help investors make in-\nformed decisions and predict market trends.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":307,"to":317}}}}],["993c4ee9-2321-4333-a171-8be5ee8f6d18",{"pageContent":"news articles and the behavior of the financial market. This can help investors make in-\nformed decisions and predict market trends. \nFirst, we presented the improved emotional annotation algorithm expanding the vo-\ncabulary, expanding categorization, and incorporating a financial glossary (domain-spe-\ncific  corpus). We then covered the  development  of  the  news  co-occurrence  network by \ndownloading financial news and market financial data and analyzing those articles across \nmarket events categorized by the emotional annotation algorithm. We used this approach \nto find relationships between companies and their sectors based on daily emotional anno-\ntation occurring during market events. \n3.1. Emotional Annotation Algorithm \nTo be able to analyze the financial news, we needed an algorithm that included fi-\nnancial terms, an improved language corpus with additional words, the ability to incor-\nporate multi-word phrases, and an expanded emotional dialect (30 emotions). The origi-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":317,"to":329}}}}],["16702eb6-e3db-4fb6-8c69-3da265138e31",{"pageContent":"nancial terms, an improved language corpus with additional words, the ability to incor-\nporate multi-word phrases, and an expanded emotional dialect (30 emotions). The origi-\nnal algorithm had five primary emotions, whereas the NRC dataset included eight based \non Plutchik’s model, requiring us to normalize the Text2Emotion embedded corpus. We \nalso expanded to include the addition of 22 mixed emotions and improvements to lever-\nage  sentiment  to  emphasize  the  calculation.  Logical  Model  1  shows  the  high-level  im-\nprovements to the original Text2Emotion algorithm from 5 to 8 emotions, expanded cor-\npus,  and  sentiment  improvements.  Logical  Model  2  shows  leveraging  the  new  logical \nmodel  1  to  annotate  the  domain  specific  knowledge  to  then  expand  domain  specific \nknowledge into the corpus to include phrases and further expanding emotion annotation \nwith the remaining 22 emotions. \n \nLogical Model 1—Improving the emotional algorithm. \nCorpus Datasets","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":329,"to":342}}}}],["c5ce8613-3302-4200-98d9-aa3796a4caf3",{"pageContent":"knowledge into the corpus to include phrases and further expanding emotion annotation \nwith the remaining 22 emotions. \n \nLogical Model 1—Improving the emotional algorithm. \nCorpus Datasets \nThe Text2Emotion with normalized emotions to match the NRC dataset, the addition \nof  the  NRC  Emotion  Lexicon  dataset  and  the  addition  of  domain  specific  (financial \nphrases) glossary were merged into a final corpus to process the financial news articles. \nNRC   EmoLex: https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm (ac-\ncessed on 24 April 2022) \nExpand Emotions\nExpand Language \nCorpus\nDomain specific \nterms\nImprove Data \nCleansing\nAdd sentiment \nadjustment (text \ncontext)\nLogical Model 1—Improving the emotional algorithm.\nCorpus Datasets\nThe Text2Emotion with normalized emotions to match the NRC dataset, the addition\nof the NRC Emotion Lexicon dataset and the addition of domain specific (financial phrases)\nglossary were merged into a final corpus to process the financial news articles.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":342,"to":366}}}}],["33fe27d9-e89a-475c-ab4f-e502c9d4b814",{"pageContent":"of the NRC Emotion Lexicon dataset and the addition of domain specific (financial phrases)\nglossary were merged into a final corpus to process the financial news articles.\nNRC EmoLex:https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm (ac-\ncessed on 24 April 2022)\nThe NRC Emotion Lexicon is a list of words and their associations with eight emotions\nbased on the Plutchik (2001) model (anger, fear, anticipation, trust, surprise, sadness, joy,\nand disgust) and two sentiments (negative and positive).  The annotations were man-\nually conducted using Amazon’s Mechanical Turk, a crowdsourcing marketplace with\n6466 words.\nEmoLex was chosen as the dataset for emotional analysis for several reasons. Firstly,\nit includes a comprehensive list of words with their emotional values, providing a solid\nfoundation for sentiment analysis. Secondly, the words in the EmoLex dataset are annotated","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":366,"to":377}}}}],["06d78afb-e2b4-42eb-888a-3dc7e0aa9c21",{"pageContent":"J. Risk Financial Manag.2023,16, 2266 of 19\nbased on Plutchik’s Wheel of Emotions, which provides a rich, multidimensional approach\nto understanding emotions beyond just positive and negative.  This allows for a more\nnuanced understanding of emotions, which is particularly useful in financial analysis,\nwhere the difference between mild concern and deep-seated anxiety can have significant\nimplications for investment decisions. Finally, EmoLex has been widely used and validated\nin academic research and industry, which increases its credibility and reliability. Overall, the\nEmoLex dataset is a valuable tool for emotional analysis in various fields, including finance,\nmarketing, and social sciences. Leveraging this improved corpus to then translate financial\nphrases allows unique financial phrases to be incorporated into a more accurate analysis.\nText2Emotion:https://github.com/aman2656/text2emotion-library  (accessed  on  24\nApril 2022)","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":379,"to":390}}}}],["43cc72fe-6de0-478e-bcb5-d720b5d68e7e",{"pageContent":"phrases allows unique financial phrases to be incorporated into a more accurate analysis.\nText2Emotion:https://github.com/aman2656/text2emotion-library  (accessed  on  24\nApril 2022)\nThis library contained an embedded corpus that was externalized and represented\n8666 words.\nThis library was compatible with 5 different emotion categories, happy, angry, sad,\nsurprise, and fear; these emotions were normalized to the Plunkett emotion model so that\nthe following words were mapped to the standard emotional term in the Plutchik emotional\nwheel (happy -> joy, angry -> anger, sad -> sadness, surprise -> surprise, and fear -> fear).\nIn our Algorithm1:  “get_emotion” that built the dictionary of emotions, we made\nseveral enhancements:\n•We added sentiment checks leveraging the Natural Language Toolkit (NLTK) (Bird\net al. 2009), which uses certain rules to incorporate the impact of surrounding text on\nperceived sentiment to slightly adjust those emotions that aligns to sentiment. The","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":390,"to":403}}}}],["c2c65ead-81f5-4019-970a-9e9b06699071",{"pageContent":"et al. 2009), which uses certain rules to incorporate the impact of surrounding text on\nperceived sentiment to slightly adjust those emotions that aligns to sentiment. The\nalgorithm, called VADER (valence aware dictionary and sentiment reasoner) (Hutto\nand Gilbert 2014), is a lexicon-based sentiment analysis tool that uses a rule-based\napproach to determine the sentiment of a piece of text.   It uses a combination of\nsentiment lexicons, grammatical rules, and syntactical patterns to assign a sentiment\nscore to the text. The formula for calculating the sentiment score using VADER can be\nrepresented as:\nSentiment score = (WPS∗Valence) + (SPS∗Intensity) + EmoticonScore\n•As shown in the algorithm, additional data cleansing (Table 1) was also added to the\nText2Emotion library to support better matching of terms, leveraging lemmatization\nover stemming a more modern approach and leveraging NLTKs updated stop words\nvocabulary all with the intent of achieving better term matching with the larger corpus.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":403,"to":415}}}}],["96149917-9bc3-4e0c-923f-b3803d5f26f1",{"pageContent":"over stemming a more modern approach and leveraging NLTKs updated stop words\nvocabulary all with the intent of achieving better term matching with the larger corpus.\n•In analyzing articles, we found that using just the standard aggregate of individual\nwords affect results in some articles being classified incorrectly. This led to the intuition\nof making just slight adjustments based on the overall article sentiment. If we find a\npositive sentiment and the emotion is positive (trust, surprise, joy, or anticipation) then\nthe calculation of the word (how much that word contributes to the overall emotion) is\nslightly adjusted by an extra 0.5. The negative sentiment similarly adjusts the negative\nemotions (fear, anger, sadness, or disgust). This basically provides an emphasis on the\nemotion based on polarity of the overall article being analyzed. We tried larger values\n(adjusting by 1) and smaller values (adjusting by 0.25) that showed little difference","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":415,"to":425}}}}],["be9c38d7-c238-4376-9ab8-b631666a086f",{"pageContent":"emotion based on polarity of the overall article being analyzed. We tried larger values\n(adjusting by 1) and smaller values (adjusting by 0.25) that showed little difference\nbefore arriving at 0.5. This adjustment created stronger separation of emotions.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":425,"to":427}}}}],["1f79b99e-090d-4a50-bd6b-dc33776d5071",{"pageContent":"J. Risk Financial Manag.2023,16, 2267 of 19\nTable 1.Data cleansing.\nData CleansingActionDescription\nstandardize_accented_charsAddedStandardize accent characters\nexpand_contractionsUpgradedExpand language contractions\nremoving_notKeptNo changes\nlemmatizationUpgradedChanged from stemming to lemmatization\nremoving_stopwordsUpgradedUpgraded to NLTK to remove stopwords\nremoving_shortcutsKeptRemoved emojis and shortcuts\nThis intuition was used to adjust the previous Text2Emotion algorithm by adding to\nthe emotion value by +0.5 as:\nEmotion vector =\n∑\n(emo∈emotions_list) emotions[emo] = emotions[emo]+ 1\nif sentiment == ‘positive’:\nif emo in [‘trust’, ‘surprise’, ‘joy’, ‘anticipation’]:\nemotions[emo] += 0.5\nelif sentiment == ‘negative’:\nif emo in [‘fear’, ‘anger’, ‘sadness’, ‘disgust’]:\nemotions[emo] += 0.5\nThe result vector is normalized so all emotions for the sentence are normalized to\n100% as:\nNormalize emotion vector =\n∑\n(i∈emotions) emotion_values[i] =\nround(emotions[i]/\n∑\nk∈emotions emotions[k], 2)","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":429,"to":456}}}}],["ede0f88f-0bed-4dbd-acd1-81c23c8a5d06",{"pageContent":"The result vector is normalized so all emotions for the sentence are normalized to\n100% as:\nNormalize emotion vector =\n∑\n(i∈emotions) emotion_values[i] =\nround(emotions[i]/\n∑\nk∈emotions emotions[k], 2)\nThese are then leveraged within the updated algorithm (get_emotion) to return the\nemotion vector for the articles.\nAlgorithm 1Obtain the emotion vector for the news article\nAlgorithm: get_emotion\nInput: News Article\nOutput: Dictionary ‘emotions’\n1. CalculateSentiment scorefor the article\n2.Clean input(remove stopwords, lemmatization, remove shortcuts, expand contradiction)\n3. Createword to emotionslexicon of financial phrases and NRC EmoLex and Text2Emotion\nword to emotion mappings and store the data in the dictionary ‘data’\n4. Initialize anEmotion vector‘emotions’ with keys “fear”, “anger”, “trust”, “surprise”,\n“sadness”, “disgust”, “joy”, and “anticipation”, all set to 0\n5. For each word in the ‘Article’, do the following:\na.    UpdateEmotion vectorsbased on emotion in word to emotion lexicon.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":456,"to":477}}}}],["d0a04b5f-4150-4135-95aa-477a33012c94",{"pageContent":"“sadness”, “disgust”, “joy”, and “anticipation”, all set to 0\n5. For each word in the ‘Article’, do the following:\na.    UpdateEmotion vectorsbased on emotion in word to emotion lexicon.\n6.Normalize emotion vector\n7. Return ‘emotion vector’\nThe merged values of Text2Emotion corpora and NRC resulted in a combined corpora\nof 13,470 with overlap of 1664 words where the emotions were merged between the two\nsets.  This new merged corpus and improved library was then used to process financial\nphrases from Investopedia.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 8 of 19 \n \n \n \nLogical Model 2—Improved Emotion algorithm with financial phrases \nInvestopedia: https://www.investopedia.com/financial-term-dictionary-4769738 (ac-\ncessed on 20, June 2022) \nTo  expand  the  lexicon  to  include  financial  phrases  we  crawled  the  financial  terms \nfrom the Investopedia glossary and pulled down the articles that describe the financials","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":477,"to":494}}}}],["11a21d25-3fca-4e95-a943-07ed6e59010e",{"pageContent":"To  expand  the  lexicon  to  include  financial  phrases  we  crawled  the  financial  terms \nfrom the Investopedia glossary and pulled down the articles that describe the financials \nterms  and  ran  that  through  the  Text2Emotion+NRC  merged  corpora  to  generate  a  new \nfinancial phrase corpus, representing 6253 financial terms. The financial terms are in some \ncases multi-word phrases (e.g., accelerated depreciation); as such, we needed to update \nthe  library to  also  support  phrases  incorporating a  look-ahead  vector  based  on  the  first \nwork and looking forward in the sentence for multiword phrase matching. When the sin-\ngle word is not found we also look ahead to see if the phrase from start word exists so that \nphrases can also be leveraged in this case financial terms. \nThe new algorithm, Algorithm2 “get_mixed_emotion”, described next, was then ca-\npable of matching financial phrases based on the financial phrase corpora. Furthermore,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":494,"to":504}}}}],["f35b15d3-84a9-4c25-aa41-b179f57d35ab",{"pageContent":"The new algorithm, Algorithm2 “get_mixed_emotion”, described next, was then ca-\npable of matching financial phrases based on the financial phrase corpora. Furthermore, \nwe adapted to incorporate the emotional mixing (Plutchik 2001) (including 22 new emo-\ntions). When the top two emotions represent 50% of the emotional calculation and the two \nemotions are within 15%, then the mixed emotion is returned based on Table 2 (e.g., joy + \ntrust -> love) and the “get_mixed_emotion” algorithm. \nTable 2. Plutchik emotion mixing. \nMixed Emotion Top 2 Emotions Mixed Emotion Top 2 Emotions \nLove Joy + trust Remorse Sadness + disgust \nGuilt Joy + fear Envy Sadness + anger \nDelight Joy + surprise Pessimism Sadness + anticipation \nSubmission Trust + fear Contempt Disgust + anger \nCuriosity Trust + surprise Cynicism Disgust + anticipation \nSentimentality Trust + sadness Morbidity Disgust + joy \nAwe Fear + surprise Aggression Anger + anticipation \nDespair Fear + sadness Pride Anger + joy","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":504,"to":519}}}}],["78a9f746-37f4-430a-b8f1-8f83b3d20239",{"pageContent":"Sentimentality Trust + sadness Morbidity Disgust + joy \nAwe Fear + surprise Aggression Anger + anticipation \nDespair Fear + sadness Pride Anger + joy \nShame Fear + disgust Dominance Anger + trust \nDisappointment Surprise + sadness Optimism Anticipation + joy \nUnbelief Surprise + disgust Hope Anticipation + trust \nOutrage Surprise + anger Anxiety Anticipation + fear \n \nAlgorithm 2 Obtain the top emotion or mixed emotion from Plutchik emotion mixing \nAlgorithm: get_mixed_emotion \nInput: News Article \nOutput: Top Emotion (or Mixed Emotion) \n1. Call get_emotion to get normalized vector of emotions for the article \n2. Sort ‘emotions’ vector by descending value  \n3. If the sum of the top two values is greater than 0.5 and the difference between the \nvalues of the top two emotions is within 0.15, do the following:  \n     Return the emotion from Table 2 \n4. Otherwise, return the top emotion from the vector \n  \nLogical Model 1Add domain corpusAdd Emotion Mixing","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":519,"to":538}}}}],["5da04c49-64ab-49ec-a213-3c8fc517a515",{"pageContent":"Return the emotion from Table 2 \n4. Otherwise, return the top emotion from the vector \n  \nLogical Model 1Add domain corpusAdd Emotion Mixing\nLogical Model 2—Improved Emotion algorithm with financial phrases","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":538,"to":542}}}}],["83beaa3c-43b0-44e4-92be-49a72d37201a",{"pageContent":"J. Risk Financial Manag.2023,16, 2268 of 19\nInvestopedia:https://www.investopedia.com/financial-term-dictionary-4769738 (accessed\non 20 June 2022)\nTo expand the lexicon to include financial phrases we crawled the financial terms\nfrom the Investopedia glossary and pulled down the articles that describe the financials\nterms and ran that through the Text2Emotion+NRC merged corpora to generate a new\nfinancial phrase corpus, representing 6253 financial terms. The financial terms are in some\ncases multi-word phrases (e.g., accelerated depreciation); as such, we needed to update\nthe library to also support phrases incorporating a look-ahead vector based on the first\nwork and looking forward in the sentence for multiword phrase matching. When the single\nword is not found we also look ahead to see if the phrase from start word exists so that\nphrases can also be leveraged in this case financial terms.\nThe new algorithm, Algorithm2 “get_mixed_emotion”, described next, was then capa-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":544,"to":556}}}}],["f439a696-61f7-4382-a762-a42d4e00fa8e",{"pageContent":"phrases can also be leveraged in this case financial terms.\nThe new algorithm, Algorithm2 “get_mixed_emotion”, described next, was then capa-\nble of matching financial phrases based on the financial phrase corpora. Furthermore, we\nadapted to incorporate the emotional mixing (Plutchik 2001) (including 22 new emotions).\nWhen the top two emotions represent 50% of the emotional calculation and the two emo-\ntions are within 15%, then the mixed emotion is returned based on Table 2 (e.g., joy + trust\n-> love) and the “get_mixed_emotion” algorithm.\nTable 2.Plutchik emotion mixing.\nMixed EmotionTop 2 EmotionsMixed EmotionTop 2 Emotions\nLoveJoy + trustRemorseSadness + disgust\nGuiltJoy + fearEnvySadness + anger\nDelightJoy + surprisePessimismSadness + anticipation\nSubmissionTrust + fearContemptDisgust + anger\nCuriosityTrust + surpriseCynicismDisgust + anticipation\nSentimentalityTrust + sadnessMorbidityDisgust + joy\nAweFear + surpriseAggressionAnger + anticipation\nDespairFear + sadnessPrideAnger + joy","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":556,"to":572}}}}],["66ff62a2-60e7-49dd-982c-8b51ba3b42b3",{"pageContent":"CuriosityTrust + surpriseCynicismDisgust + anticipation\nSentimentalityTrust + sadnessMorbidityDisgust + joy\nAweFear + surpriseAggressionAnger + anticipation\nDespairFear + sadnessPrideAnger + joy\nShameFear + disgustDominanceAnger + trust\nDisappointmentSurprise + sadnessOptimismAnticipation + joy\nUnbeliefSurprise + disgustHopeAnticipation + trust\nOutrageSurprise + angerAnxietyAnticipation + fear\nAlgorithm 2Obtain the top emotion or mixed emotion from Plutchik emotion mixing\nAlgorithm: get_mixed_emotion\nInput: News Article\nOutput: Top Emotion (or Mixed Emotion)\n1. Call get_emotion to get normalized vector of emotions for the article\n2. Sort ‘emotions’ vector by descending value\n3. If the sum of the top two values is greater than 0.5 and the difference between the values of the\ntop two emotions is within 0.15, do the following:\nReturn the emotion from Table 2\n4. Otherwise, return the top emotion from the vector\n3.2. News Co-Occurrence Network","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":572,"to":590}}}}],["66e99bd6-14d5-4d3d-98b1-c627dacf6e6c",{"pageContent":"top two emotions is within 0.15, do the following:\nReturn the emotion from Table 2\n4. Otherwise, return the top emotion from the vector\n3.2. News Co-Occurrence Network\nMacroeconomic indicators with their respective data sources and timeframes were\npulled from the Federal Reserve. These include crude oil (1986–2021), inflation (2003–2021),\nCPI (1947–2021), trade-weighted dollar index (2006–2021), real gross domestic product\n(1947–2021), unemployment (1948–2021), and recession data (1854–2021). Each indicator\nhas a specific measurement and provides insight into various aspects of the US economy,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":590,"to":598}}}}],["e6214bf3-d435-4c39-8d85-42d11570dddf",{"pageContent":"J. Risk Financial Manag.2023,16, 2269 of 19\nsuch as inflation, consumer buying habits, GDP, and business cycles.  The data can be\nused to analyze the performance of different sectors and asset classes over time.  The\nfollowing are the sectors and indicators of exchange-traded funds (ETFs) in the study:\nenergy,  gold miners,  materials,  industrials,  consumer discretionary,  consumer staples,\nhealth care, financials, technology, telecommunication, utilities, real estate, and the S&P 500.\nArticles (Table 3) were downloaded for each of the top 10 companies in each sector\nfrom 16 top financial news sources including conservative, liberal, and international. Each\narticle was processed to capture concept (name of the company) through its Wikipedia\nreference (Table A1) to ensure exact match on company, sentiment and post processed\nto add in the top emotion based on the new emotion algorithm library we updated as","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":600,"to":610}}}}],["c4d36444-b389-4732-be1c-43d888d2c6b2",{"pageContent":"reference (Table A1) to ensure exact match on company, sentiment and post processed\nto add in the top emotion based on the new emotion algorithm library we updated as\npart of this research.   Together,  a new dataset was created containing the sector,  date,\npercent_change, and sentiment_daily across all sectors (all_news_sentiments.csv).\nTable 3.Sectors and Articles.\nETFSectorCompaniesArticles\nXLEEnergyTop 10 by holdings8671\nGDXGold minersTop 10 by holdings1514\nXLBMaterialsTop 10 by holdings25,526\nDIAIndustrialsTop 10 by holdings78,215\nXLYConsumer discretionaryTop 10 by holdings129,685\nXLPConsumer staplesTop 10 by holdings36,753\nXLVHealth careTop 10 by holdings28,607\nXLFFinancialsTop 10 by holdings71,310\nXLKTechnologyTop 10 by holdings109,429\nIYZTelecommunicationTop 10 by holdings22,499\nXLUUtilitiesTop 10 by holdings2874\nVNQReal estateTop 10 by holdings1890\n516,973 articles\nLeveraging a data analysis library ‘pandas’, we created a new dataset to store date,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":610,"to":629}}}}],["e94e8dff-da8c-4da4-a394-40d4f83fd2b0",{"pageContent":"XLUUtilitiesTop 10 by holdings2874\nVNQReal estateTop 10 by holdings1890\n516,973 articles\nLeveraging a data analysis library ‘pandas’, we created a new dataset to store date,\nsector, price, and percentage change from the previous day. This data was merged with\nthe news for that sector and average daily sentiment for that sector. This allowed for the\nanalysis of market movements±2% across 1 January 2019 through 1 January 2021.\nNews articles were analyzed and merged into a final dataset that included the date,\nsector, company, percentage change from previous day, and sentiment daily mean for\nfurther analysis.\nWe then conducted standard correlation coefficients analysis on sector time series\ninformation (using sentiment alone) to validate if the generated co-occurrence network\ndescribed next shows the same market reaction (looking at relations > 0.55). The goal is\nto see if the significant pairs of companies from the news network moved together (up or\ndown) in market.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":629,"to":643}}}}],["220cf824-74d7-462e-83a2-a9a64a2d208c",{"pageContent":"described next shows the same market reaction (looking at relations > 0.55). The goal is\nto see if the significant pairs of companies from the news network moved together (up or\ndown) in market.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 10 of 19 \n \n \n \nLogical Model 3—Generating sentiment across all companies and sectors. \n \nLogical Model 4—Financial news co-occurrence graph \nTo create the financial news co-occurrence network, see Algorithm3 described next, \nwe  used the  combined  dataset  that  was  postprocessed  in  the  final  dataset.  If the  same \ncompany pair was found with the same emotion, that emotion was tracked for that com-\npany pair. The weight of the edge was then the sum of all emotions for that company pair \nup to an upper bound of 27 (which represents the number of unique emotions discovered \nthrough all the news articles). Edges were only created with weights > 5. \nAlgorithm 3 Create the co-occurrence new network for company pairs","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":643,"to":659}}}}],["89b641e0-376f-4fa7-9ff5-236e7a8f436e",{"pageContent":"through all the news articles). Edges were only created with weights > 5. \nAlgorithm 3 Create the co-occurrence new network for company pairs \nAlgorithm: Co-occurrence Network Construction \nInput: Dataset including date, company, and emotion for each news article for that day \nOutput: Co-occurrence graph network constructed from the emotion data (pair of com-\npanies that share the same emotion on the same day) \n1. Generate Edges \nforeach emotion in emotions: \n       group = from dataset get companies with the same emotion by date (group by date, \nemotion) \n       for each row in group \n                 Set edges = Get all combination of companies with the same emotion  \n                 Foreach edge in edges \n                       If edge not in hash set edge to empty {} \n                       edge_hash[edge][emo]  =  if first  time  emotion  seen  for  edge, initialize  to \nzero  \n                       edge_hash[edge][emo] += 1                        \n2. Generate Nodes with all Companies Names","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":659,"to":676}}}}],["fc5c4731-1a5b-421c-98cf-36d4a42c4702",{"pageContent":"zero  \n                       edge_hash[edge][emo] += 1                        \n2. Generate Nodes with all Companies Names \nGraph.add_nodes_from(list of companies) \n3. Keep edge weights greater than 5 \n Foreach key in edges \n       If weight > 5  \n                 Graph.Add(key) \n4. Results and Discussion \nWe found that by expanding the vocabulary to include (4804) additional terms and \nadding support for financial phrases allows for improved analysis of financial news arti-\ncles. The new corpus increased the original library by 224%, greatly expanding the avail-\nable vocabulary with 6253 financial terms and phrases to a new combined corpus of 19,430 \nterms (Figure 1). These (10,764) additional terms provide a significant improved vocabu-\nlary including the ability to recognize financial phrases. This same approach could be aug-\nmented to any domain where a glossary is available for domain-specific analysis. \nData\n•Sector values \n(with % change)\n•Finanical News\nGroup by Article \nand Sector","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":676,"to":697}}}}],["62cacde8-9fcf-4e06-b245-68bbe4507ca8",{"pageContent":"mented to any domain where a glossary is available for domain-specific analysis. \nData\n•Sector values \n(with % change)\n•Finanical News\nGroup by Article \nand Sector\nProcess article by \nsector, concept, \nsentiment\nAnnotate with \nemotion\nCalculate mean \nsentiment daily\nMerge\n•Sector News \nSentiment\n•Company Emotion \nNews Annotation\nCalculate Sector Percent \nChange\nFilter on Significant \nevents (2%)\nGroup by Date, \nEmotion, Company\nGenerate Co-Occurence \nGraph\nLogical Model 3—Generating sentiment across all companies and sectors.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 10 of 19 \n \n \n \nLogical Model 3—Generating sentiment across all companies and sectors. \n \nLogical Model 4—Financial news co-occurrence graph \nTo create the financial news co-occurrence network, see Algorithm3 described next, \nwe  used the  combined  dataset  that  was  postprocessed  in  the  final  dataset.  If the  same \ncompany pair was found with the same emotion, that emotion was tracked for that com-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":697,"to":734}}}}],["6d587719-d0bf-4969-9941-aad76e5c0b1a",{"pageContent":"we  used the  combined  dataset  that  was  postprocessed  in  the  final  dataset.  If the  same \ncompany pair was found with the same emotion, that emotion was tracked for that com-\npany pair. The weight of the edge was then the sum of all emotions for that company pair \nup to an upper bound of 27 (which represents the number of unique emotions discovered \nthrough all the news articles). Edges were only created with weights > 5. \nAlgorithm 3 Create the co-occurrence new network for company pairs \nAlgorithm: Co-occurrence Network Construction \nInput: Dataset including date, company, and emotion for each news article for that day \nOutput: Co-occurrence graph network constructed from the emotion data (pair of com-\npanies that share the same emotion on the same day) \n1. Generate Edges \nforeach emotion in emotions: \n       group = from dataset get companies with the same emotion by date (group by date, \nemotion) \n       for each row in group","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":734,"to":748}}}}],["151dc5aa-50ff-4ab0-8727-b9feedf9b81f",{"pageContent":"1. Generate Edges \nforeach emotion in emotions: \n       group = from dataset get companies with the same emotion by date (group by date, \nemotion) \n       for each row in group \n                 Set edges = Get all combination of companies with the same emotion  \n                 Foreach edge in edges \n                       If edge not in hash set edge to empty {} \n                       edge_hash[edge][emo]  =  if first  time  emotion  seen  for  edge, initialize  to \nzero  \n                       edge_hash[edge][emo] += 1                        \n2. Generate Nodes with all Companies Names \nGraph.add_nodes_from(list of companies) \n3. Keep edge weights greater than 5 \n Foreach key in edges \n       If weight > 5  \n                 Graph.Add(key) \n4. Results and Discussion \nWe found that by expanding the vocabulary to include (4804) additional terms and \nadding support for financial phrases allows for improved analysis of financial news arti-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":748,"to":767}}}}],["5a7be64f-806a-4715-925d-d21e53147ed4",{"pageContent":"4. Results and Discussion \nWe found that by expanding the vocabulary to include (4804) additional terms and \nadding support for financial phrases allows for improved analysis of financial news arti-\ncles. The new corpus increased the original library by 224%, greatly expanding the avail-\nable vocabulary with 6253 financial terms and phrases to a new combined corpus of 19,430 \nterms (Figure 1). These (10,764) additional terms provide a significant improved vocabu-\nlary including the ability to recognize financial phrases. This same approach could be aug-\nmented to any domain where a glossary is available for domain-specific analysis. \nData\n•Sector values \n(with % change)\n•Finanical News\nGroup by Article \nand Sector\nProcess article by \nsector, concept, \nsentiment\nAnnotate with \nemotion\nCalculate mean \nsentiment daily\nMerge\n•Sector News \nSentiment\n•Company Emotion \nNews Annotation\nCalculate Sector Percent \nChange\nFilter on Significant \nevents (2%)\nGroup by Date, \nEmotion, Company\nGenerate Co-Occurence \nGraph","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":767,"to":800}}}}],["6b8e6956-e98a-4f3b-8f3a-3fc28df9e598",{"pageContent":"sentiment daily\nMerge\n•Sector News \nSentiment\n•Company Emotion \nNews Annotation\nCalculate Sector Percent \nChange\nFilter on Significant \nevents (2%)\nGroup by Date, \nEmotion, Company\nGenerate Co-Occurence \nGraph\nLogical Model 4—Financial news co-occurrence graph\nTo create the financial news co-occurrence network, see Algorithm3 described next, we\nused the combined dataset that was postprocessed in the final dataset. If the same company","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":857,"to":873}}}}],["145070ff-d03d-4c53-a021-2bd6811cf38b",{"pageContent":"J. Risk Financial Manag.2023,16, 22610 of 19\npair was found with the same emotion, that emotion was tracked for that company pair.\nThe weight of the edge was then the sum of all emotions for that company pair up to an\nupper bound of 27 (which represents the number of unique emotions discovered through\nall the news articles). Edges were only created with weights > 5.\nAlgorithm 3Create the co-occurrence new network for company pairs\nAlgorithm: Co-occurrence Network Construction\nInput: Dataset including date, company, and emotion for each news article for that day\nOutput\n: Co-occurrence graph network constructed from the emotion data (pair of companies that\nshare the same emotion on the same day)\n1. Generate Edges\nforeach emotion in emotions:\ngroup = from dataset get companies with the same emotion by date (group by date, emotion)\nfor each row in group\nSet edges = Get all combination of companies with the same emotion\nForeach edge in edges\nIf edge not in hash set edge to empty {}","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":875,"to":892}}}}],["853124be-a59c-4069-8df3-7f6fd7eb152a",{"pageContent":"for each row in group\nSet edges = Get all combination of companies with the same emotion\nForeach edge in edges\nIf edge not in hash set edge to empty {}\nedge_hash[edge][emo] = if first time emotion seen for edge, initialize to zero\nedge_hash[edge][emo] += 1\n2. Generate Nodes with all Companies Names\nGraph.add_nodes_from(list of companies)\n3. Keep edge weights greater than 5\nForeach key in edges\nIf weight > 5\nGraph.Add(key)\n4. Results and Discussion\nWe found that by expanding the vocabulary to include (4804) additional terms and\nadding support for financial phrases allows for improved analysis of financial news articles.\nThe new corpus increased the original library by 224%, greatly expanding the available\nvocabulary with 6253 financial terms and phrases to a new combined corpus of 19,430 terms\n(Figure 1). These (10,764) additional terms provide a significant improved vocabulary in-\ncluding the ability to recognize financial phrases. This same approach could be augmented","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":892,"to":910}}}}],["1fcdeba2-226c-4b75-8407-30d4a8dbfb28",{"pageContent":"(Figure 1). These (10,764) additional terms provide a significant improved vocabulary in-\ncluding the ability to recognize financial phrases. This same approach could be augmented\nto any domain where a glossary is available for domain-specific analysis.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 11 of 19 \n \n \n \nFigure 1. Expanded corpus. \nIn the labeling of Investopedia terms and phrases dictionary we compared how the \narticle would have been interpreted by Text2Emotion corpus with the addition of the NRC \ndataset bringing the understood vocabulary from 8666 to 13,470 with new aligned Plun-\nkett emotions and mixed emotions. Terms were considered equal using the normalized \nmechanism (happy -> joy, angry -> anger, sad -> sadness, surprise -> surprise, and fear -> \nfear). The NRC dataset added in the missing emotions (trust, anticipation, and disgust). \nIn analyzing financial articles, 5702 articles resulted in the same emotion as compared to","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":910,"to":924}}}}],["edb89f17-37cf-41f2-9299-e69c63f4fcf0",{"pageContent":"fear). The NRC dataset added in the missing emotions (trust, anticipation, and disgust). \nIn analyzing financial articles, 5702 articles resulted in the same emotion as compared to \nthe original algorithm. The new algorithm for 551 articles uncovered different emotions \nnot found by the original algorithm. When the two top emotions represented over 50% of \nthe  emotional  state, both  emotions  were  included  in  the  corpus  (e.g.,  sadness  and  fear \nnoted below). \nWe then examined the differences, looking at the differences found for happy (joy in \nthe normalized model). The improved algorithm found the following differences. \n• Trust (vs. happy)—Twelve financial phrases differ (consumer goods, finance charge, \nFortune  100,  Fortune  500, free carrier, income elasticity  of demand, inferior goods, \nlegal tender, normal good, orange book, virtual good, Westpac consumer confidence \nindex); from a reader’s perspective, the articles do not read happy, these read as the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":924,"to":935}}}}],["b2487806-2766-4dfb-9b2d-df937e54cd7a",{"pageContent":"legal tender, normal good, orange book, virtual good, Westpac consumer confidence \nindex); from a reader’s perspective, the articles do not read happy, these read as the \ndefinition of financial terms. \n• Surprise (vs. happy)—Three financial phrases differ (Giffen good, one-time charge, \nvolatility smile); from  a  reader’s perspective, the  articles  do  read  more  surprise \n(Giffen good being a condition that does not follow standard economic theory, one-\ntime charges being a surprise to many, and volatility smile being a change in volatil-\nity as a surprise in economic movement) \n• Sadness, fear  (vs. happy) (representing  mixed  emotion  despair)—One financial \nphrase differs (tax evasion); the reader would also concur that this does not represent \nhappy and is more appropriately defined as despair. \n• Anticipation  (vs. happy)—Two financial  phrases  differ  (public good, rival good); \nthese articles do read more anticipation in positive outcomes (a public good being a","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":935,"to":947}}}}],["f09b7e1f-2218-4a5f-8952-d031df35034b",{"pageContent":"• Anticipation  (vs. happy)—Two financial  phrases  differ  (public good, rival good); \nthese articles do read more anticipation in positive outcomes (a public good being a \ncommodity or service provided without profit). \nThe  financial  dictionary  contained  phrases,  so  it  was  necessary  to  adjust  the  algo-\nrithm to also incorporate phrases where the original Text2Emotion was limited to single \nwords. The algorithm needed to do a partial key search based on current word and then \nlook forward into sentence for phrase (e.g., acceleration clause). \nFigure 1.Expanded corpus.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":947,"to":954}}}}],["d217b542-8cf6-4e72-96cd-236020c7c117",{"pageContent":"J. Risk Financial Manag.2023,16, 22611 of 19\nIn the labeling of Investopedia terms and phrases dictionary we compared how the\narticle would have been interpreted by Text2Emotion corpus with the addition of the\nNRC dataset bringing the understood vocabulary from 8666 to 13,470 with new aligned\nPlunkett emotions and mixed emotions. Terms were considered equal using the normalized\nmechanism (happy -> joy, angry -> anger, sad -> sadness, surprise -> surprise, and fear ->\nfear). The NRC dataset added in the missing emotions (trust, anticipation, and disgust).\nIn analyzing financial articles, 5702 articles resulted in the same emotion as compared to\nthe original algorithm. The new algorithm for 551 articles uncovered different emotions\nnot found by the original algorithm.  When the two top emotions represented over 50%\nof the emotional state, both emotions were included in the corpus (e.g., sadness and fear\nnoted below).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":956,"to":967}}}}],["fa32b564-b031-4ce7-92c2-db755542801e",{"pageContent":"not found by the original algorithm.  When the two top emotions represented over 50%\nof the emotional state, both emotions were included in the corpus (e.g., sadness and fear\nnoted below).\nWe then examined the differences, looking at the differences found for happy (joy in\nthe normalized model). The improved algorithm found the following differences.\n•Trust (vs. happy)—Twelve financial phrases differ (consumer goods, finance charge,\nFortune 100, Fortune 500, free carrier, income elasticity of demand, inferior goods,\nlegal tender, normal good, orange book, virtual good, Westpac consumer confidence\nindex); from a reader’s perspective, the articles do not read happy, these read as the\ndefinition of financial terms.\n•Surprise (vs. happy)\n—Three financial phrases differ (Giffen good, one-time charge,\nvolatility smile); from a reader ’s perspective, the articles do read more surprise (Giffen\ngood being a condition that does not follow standard economic theory,  one-time","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":967,"to":980}}}}],["0013a3a3-a4f5-47da-904f-c741258bae2e",{"pageContent":"volatility smile); from a reader ’s perspective, the articles do read more surprise (Giffen\ngood being a condition that does not follow standard economic theory,  one-time\ncharges being a surprise to many, and volatility smile being a change in volatility as a\nsurprise in economic movement)\n•Sadness,  fear  (vs.   happy)(representing  mixed  emotion  despair)—One  financial\nphrase differs (tax evasion); the reader would also concur that this does not represent\nhappy and is more appropriately defined as despair.\n•Anticipation (vs.  happy)—Two financial phrases differ (public good, rival good);\nthese articles do read more anticipation in positive outcomes (a public good being a\ncommodity or service provided without profit).\nThe financial dictionary contained phrases, so it was necessary to adjust the algorithm\nto also incorporate phrases where the original Text2Emotion was limited to single words.\nThe algorithm needed to do a partial key search based on current word and then look","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":980,"to":992}}}}],["49ba0b1d-6374-410d-98bb-365ba6bed016",{"pageContent":"to also incorporate phrases where the original Text2Emotion was limited to single words.\nThe algorithm needed to do a partial key search based on current word and then look\nforward into sentence for phrase (e.g., acceleration clause).\nFor example, the following summary “The full impact of the arbitration has now been\naccounted for. The dispute relates to the years 2019 and 2020 and does not affect companies’\npositive  long-term  business  outlook  and  guidance”  was  found  as  fear  in  the  original\nalgorithm and in the new algorithm the mixed emotion was submission (combination of\ntrust and fear), providing a more accurate emotional tone of the article. To account for the\nimpact of extreme news we included mainstream financial sources (Forbes, Bloomberg),\nconservative (fox), liberal (msn), and international news sources (BBC); however, further\nanalysis on impact of more extreme news on the results should be analyzed with specific\nconsumers of those news sources.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":992,"to":1003}}}}],["06caf45c-7584-4cb9-be4f-8cb566b3a649",{"pageContent":"analysis on impact of more extreme news on the results should be analyzed with specific\nconsumers of those news sources.\nLeveraging the improved Text2Emotion algorithm and processed against all the finan-\ncial news data, we found 23 unique emotions across the possible 30 (8 primary, 22 mixed\nemotions). With fear, submission, trust, despair, surprise, anxiety, joy, awe, anticipation,\nand sadness representing the majority (Figure 2).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1003,"to":1008}}}}],["a87c5278-7ada-4d57-97dc-bd9d062f3fc6",{"pageContent":"J. Risk Financial Manag.2023,16, 22612 of 19\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 12 of 19 \n \n \nFor example, the following summary “The full impact of the arbitration has now been \naccounted for. The dispute relates to the years 2019 and 2020 and does not affect compa-\nnies’ positive long-term business outlook and guidance” was found as fear in the original \nalgorithm and in the new algorithm the mixed emotion was submission (combination of \ntrust and fear), providing a more accurate emotional tone of the article. To account for the \nimpact of extreme news we included mainstream financial sources (Forbes, Bloomberg), \nconservative (fox), liberal (msn), and international news sources (BBC); however, further \nanalysis on impact of more extreme news on the results should be analyzed with specific \nconsumers of those news sources. \nLeveraging the improved Text2Emotion algorithm and processed against all the fi-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1010,"to":1023}}}}],["88f667c8-89c9-489b-a6b4-a9a0fc891c48",{"pageContent":"consumers of those news sources. \nLeveraging the improved Text2Emotion algorithm and processed against all the fi-\nnancial  news  data, we  found  23  unique  emotions  across  the  possible  30  (8  primary,  22 \nmixed emotions). With fear, submission, trust, despair, surprise, anxiety, joy, awe, antici-\npation, and sadness representing the majority (Figure 2). \n \nFigure 2. 23 emotions (fear, submission, trust, despair, anxiety, surprise, awe, joy, anticipation, sad-\nness). \nThe first step in understanding the data is looking at the correlation. Figure 3 shows \nsentiment  by  sector  (Table  3)  and  percentage  change  from  the  previous  day  where  the \npercentage change is a change of ±2%. \n \n0\n5\n0\n1\n0\n0\n1\n5\n0\n2\n0\n0\n2\n5\n0\n3\n0\n0\n3\n5\n0\n4\n0\n0\nf\ne\na\nr\ns\nu\nb\nm\ni\ns\ns\ni\no\nn\nt\nr\nu\ns\nt\nd\ne\ns\np\na\ni\nr\na\nn\nx\ni\ne\nt\ny\ns\nu\nr\np\nr\ni\ns\ne\na\nw\ne\nj\no\ny\na\nn\nt\ni\nc\ni\np\na\nt\ni\no\nn\ns\na\nd\nn\ne\ns\ns\ng\nu\ni\nl\nt\nh\no\np\ne\nl\no\nv\ne\nd\ni\ns\na\np\np\no\ni\nn\nt\nm\ne\nn\nt\na\nn\ng\ne\nr\no\np\nt\ni\nm\ni\ns\nm\nd\no\nm\ni\nn\na\nn\nc\ne\ns\nh\na\nm\ne\np\ne\ns\ns\ni\nm\ni\ns\nm\np\nr\ni\nd\ne\no\nu\nt\nr\na\ng\ne\nd\ni\ns\ng\nu\ns\nt\nd\ne\nl\ni\ng\nh\nt\nE\nm\no\nt\ni\no\nn\n \nC\no\nu\nn\nt\ns\nFigure  2.23  emotions  (fear,  submission,  trust,  despair,  anxiety,  surprise,  awe,  joy,  anticipa-","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1023,"to":1228}}}}],["f2502ea5-7fd5-4203-ac45-e8c371dcf291",{"pageContent":"e\nl\no\nv\ne\nd\ni\ns\na\np\np\no\ni\nn\nt\nm\ne\nn\nt\na\nn\ng\ne\nr\no\np\nt\ni\nm\ni\ns\nm\nd\no\nm\ni\nn\na\nn\nc\ne\ns\nh\na\nm\ne\np\ne\ns\ns\ni\nm\ni\ns\nm\np\nr\ni\nd\ne\no\nu\nt\nr\na\ng\ne\nd\ni\ns\ng\nu\ns\nt\nd\ne\nl\ni\ng\nh\nt\nE\nm\no\nt\ni\no\nn\n \nC\no\nu\nn\nt\ns\nFigure  2.23  emotions  (fear,  submission,  trust,  despair,  anxiety,  surprise,  awe,  joy,  anticipa-\ntion, sadness).\nThe first step in understanding the data is looking at the correlation. Figure 3 shows\nsentiment by sector (Table 3) and percentage change from the previous day where the\npercentage change is a change of±2%.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 12 of 19 \n \n \nFor example, the following summary “The full impact of the arbitration has now been \naccounted for. The dispute relates to the years 2019 and 2020 and does not affect compa-\nnies’ positive long-term business outlook and guidance” was found as fear in the original \nalgorithm and in the new algorithm the mixed emotion was submission (combination of \ntrust and fear), providing a more accurate emotional tone of the article. To account for the","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1228,"to":1335}}}}],["75b38e82-00e3-4cb2-8a39-59b0dbca7895",{"pageContent":"algorithm and in the new algorithm the mixed emotion was submission (combination of \ntrust and fear), providing a more accurate emotional tone of the article. To account for the \nimpact of extreme news we included mainstream financial sources (Forbes, Bloomberg), \nconservative (fox), liberal (msn), and international news sources (BBC); however, further \nanalysis on impact of more extreme news on the results should be analyzed with specific \nconsumers of those news sources. \nLeveraging the improved Text2Emotion algorithm and processed against all the fi-\nnancial  news  data, we  found  23  unique  emotions  across  the  possible  30  (8  primary,  22 \nmixed emotions). With fear, submission, trust, despair, surprise, anxiety, joy, awe, antici-\npation, and sadness representing the majority (Figure 2). \n \nFigure 2. 23 emotions (fear, submission, trust, despair, anxiety, surprise, awe, joy, anticipation, sad-\nness).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1335,"to":1347}}}}],["c4cb8841-23dd-42a9-b745-d3aeb530b5e7",{"pageContent":"pation, and sadness representing the majority (Figure 2). \n \nFigure 2. 23 emotions (fear, submission, trust, despair, anxiety, surprise, awe, joy, anticipation, sad-\nness). \nThe first step in understanding the data is looking at the correlation. Figure 3 shows \nsentiment  by  sector  (Table  3)  and  percentage  change  from  the  previous  day  where  the \npercentage change is a change of ±2%. \n \n0\n5\n0\n1\n0\n0\n1\n5\n0\n2\n0\n0\n2\n5\n0\n3\n0\n0\n3\n5\n0\n4\n0\n0\nf\ne\na\nr\ns\nu\nb\nm\ni\ns\ns\ni\no\nn\nt\nr\nu\ns\nt\nd\ne\ns\np\na\ni\nr\na\nn\nx\ni\ne\nt\ny\ns\nu\nr\np\nr\ni\ns\ne\na\nw\ne\nj\no\ny\na\nn\nt\ni\nc\ni\np\na\nt\ni\no\nn\ns\na\nd\nn\ne\ns\ns\ng\nu\ni\nl\nt\nh\no\np\ne\nl\no\nv\ne\nd\ni\ns\na\np\np\no\ni\nn\nt\nm\ne\nn\nt\na\nn\ng\ne\nr\no\np\nt\ni\nm\ni\ns\nm\nd\no\nm\ni\nn\na\nn\nc\ne\ns\nh\na\nm\ne\np\ne\ns\ns\ni\nm\ni\ns\nm\np\nr\ni\nd\ne\no\nu\nt\nr\na\ng\ne\nd\ni\ns\ng\nu\ns\nt\nd\ne\nl\ni\ng\nh\nt\nE\nm\no\nt\ni\no\nn\n \nC\no\nu\nn\nt\ns\nFigure 3.Daily Sentiment for each sector.\nUsing the sentiment data along those events where the sectors moved more than±2%\non the previous day, we looked for any correlations between the sentiments and noted\nthat aggregated sentiment for those sectors show moderate correlation between consumer\ndiscretionary, consumer staples, financials, industrials, and technology (as noted in Table 4).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1565,"to":1770}}}}],["c033825e-02e1-4e84-8364-68b3f9ac8d1a",{"pageContent":"J. Risk Financial Manag.2023,16, 22613 of 19\nTable 4.Correlation between sectors.\nSectorCorrelationCorrelationCorrelationCorrelationCorrelation\nCons. disc.10.550.610.580.56\nCons. stap.0.5510.50.380.44\nFinancials0.610.510.630.56\nIndustrials0.580.380.6310.57\nTechnology0.560.440.560.571\nCons. disc.Cons. stap.FinancialsIndustrialsTechnology\nIn Figure 4 below, we analyze the constructed co-occurrence news graph data for\nweights greater or equal to 10 and a node degree of 10 (meaning that many different\nedges connecting different pair).  Just as we noted in sentiment correlation, we see that\ntechnology, consumer discretionary, consumer staples, and financials correlate between\nthe companies in those sectors. Notable edges across sectors (not within the same sector)\nare shown in Table 5 below. These specific pairs outside of the same sector were chosen\nto see the influence of emotional categorization of financial news and their corresponding\nmarket movements.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1772,"to":1788}}}}],["3f2c1ee5-7b3d-403a-8a53-7c6a47760a11",{"pageContent":"are shown in Table 5 below. These specific pairs outside of the same sector were chosen\nto see the influence of emotional categorization of financial news and their corresponding\nmarket movements.\nJ. Risk Financial Manag. 2023, 16, x FOR PEER REVIEW 13 of 19 \n \n \nFigure 3. Daily Sentiment for each sector. \nUsing the sentiment data along those events where the sectors moved more than ±2% \non the previous day, we looked for any correlations between the sentiments and noted \nthat aggregated sentiment for those sectors show moderate correlation between consumer \ndiscretionary, consumer staples, financials, industrials, and technology (as noted in Table \n4).\n \nTable 4. Correlation between sectors. \nSector Correlation Correlation Correlation Correlation Correlation \nCons. disc. 1 0.55 0.61 0.58 0.56 \nCons. stap. 0.55 1 0.5 0.38 0.44 \nFinancials 0.61 0.5 1 0.63 0.56 \nIndustrials 0.58 0.38 0.63 1 0.57 \nTechnology 0.56 0.44 0.56 0.57 1 \n  Cons. disc. Cons. stap. Financials Industrials Technology","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1788,"to":1808}}}}],["96bd9e57-bc57-4e8f-b721-83b9e79ab850",{"pageContent":"Cons. stap. 0.55 1 0.5 0.38 0.44 \nFinancials 0.61 0.5 1 0.63 0.56 \nIndustrials 0.58 0.38 0.63 1 0.57 \nTechnology 0.56 0.44 0.56 0.57 1 \n  Cons. disc. Cons. stap. Financials Industrials Technology \nIn Figure 4 below, we analyze the constructed co‐occurrence news graph data for \nweights greater or equal to 10 and a node degree of 10 (meaning that many different edges \nconnecting different pair). Just as we noted in sentiment correlation, we see that technol‐\nogy, consumer discretionary, consumer staples, and\n financials correlate between the com‐\npanies in those sectors. Notable edges across sectors (not within the same sector) are \nshown in Table 5 below. These specific pairs outside of the same sector were chosen to see \nthe influence of emotional categorization of financial news and their corresponding mar‐\nket movements. \n \nFigure 4. News co‐occurrence graph. \n  \nFigure 4.News co-occurrence graph.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1808,"to":1825}}}}],["0fa08afa-246e-4f32-9296-d1c0de706538",{"pageContent":"J. Risk Financial Manag.2023,16, 22614 of 19\nTable 5.Notable company pairs across sectors.\nCompany Pair 1Company Pair 2\nTarget (consumer discretionary)Apple (technology)\nTesla (consumer discretionary)Apple (technology)\nAmazon (consumer discretionary)Nvidia (technology)\nAmazon (consumer discretionary)JP Morgan (financials)\nWalmart (consumer staples)Apple (technology)\nMcDonalds (consumer discretionary)Apple (technology)\nGoldman Sachs (financials)Target (consumer discretionary)\nAmazon (consumer discretionary)Citigroup (financials)\nBoeing (industrials)Microsoft (technology)\nBoeing (industrials)Amazon (consumer discretionary)\nConsidering the notable pairs in Table 5 and the significant market events of±2%\nor greater, we look to see if they move together in the market. What we see is moderate\ncorrelation (above 60%) across sectors which aligns to the co-occurrence news network\ndespite lower correlation when taken against daily market price alone in Table 6 below","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1827,"to":1843}}}}],["9b59e3a0-62a7-4267-a77a-eccca87b81f9",{"pageContent":"correlation (above 60%) across sectors which aligns to the co-occurrence news network\ndespite lower correlation when taken against daily market price alone in Table 6 below\nas highlighted. As an observation, we see when looking at the pairs (Amazon–JPMorgan,\nAmazon–Citigroup, Boeing–Microsoft, and Boeing–Amazon) on significant market days\nthere is a moderate correlation of events based on the co-occurrence graph created through\nthe annotation of articles. We found that above 60% of the market events confirmed this\nmoderate correlation of events. The results show more accurate and effective analysis of\nmarket sentiment, investor behavior, and promise of improved financial risk management.\nTable 6.Notable pairs moving together compared to price only correlation.\nCompany PairMove TogetherTime Series Correlation (Daily Price)\nTarget–Apple68% (194/284)0.95\nTesla–Apple66% (190/284)0.94\nAmazon–Nvidia76% (218/284)0.97\nAmazon–JPMorgan60% (172/284)−0.23\nWalmart–Apple62% (177/284)0.92","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1843,"to":1857}}}}],["b9b40340-28e4-4e3d-9de1-26016ae944e8",{"pageContent":"Target–Apple68% (194/284)0.95\nTesla–Apple66% (190/284)0.94\nAmazon–Nvidia76% (218/284)0.97\nAmazon–JPMorgan60% (172/284)−0.23\nWalmart–Apple62% (177/284)0.92\nMcDonald’s–Apple61% (174/284)0.47\nGoldman Sachs–Apple68% (194/284)0.38\nAmazon–Citigroup62% (178/284)−0.54\nBoeing–Microsoft63% (179/284)−0.73\nBoeing–Amazon60% (172/284)−0.67\n5. Conclusions\nThis article presents two key contributions. Firstly, an improved language corpus was\nintroduced that includes financial phrases and terms to enable more accurate and in-depth\nanalysis of financial news articles. This improved corpus incorporates emotional mixing,\nresulting in 30 distinct emotions compared to the original five, which will improve data\nanalysis for researchers leveraging Text2Emotion to categorize articles based on emotional\nanalysis.  The finding is that the improved algorithm for emotional analysis of financial\nnews articles has identified differences in emotional content compared to the original","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1857,"to":1874}}}}],["07ba5c95-3970-4903-8ed2-a528ac370ed6",{"pageContent":"analysis.  The finding is that the improved algorithm for emotional analysis of financial\nnews articles has identified differences in emotional content compared to the original\nText2Emotion model. Specifically, this study found that 12 financial phrases related to trust,\n3 related to surprise, 1 related to mixed emotion despair, and 2 related to anticipation had","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1874,"to":1877}}}}],["26006ccb-eabd-4c82-a995-7006c84b3337",{"pageContent":"J. Risk Financial Manag.2023,16, 22615 of 19\nsignificant differences in emotional content compared to the original emotion of happiness.\nThe improved algorithm allows for a more nuanced analysis of financial news articles and\nprovides researchers with a better understanding of the emotional impact of financial terms\nand phrases.  This can lead to more accurate and effective analysis of market sentiment,\ninvestor behavior, and financial risk management.\nSecondly, an efficient method for creating co-occurrence networks based on emotional\nclassification  was  proposed,  which  identified  connections  between  companies  within\nand across sectors based on financial news emotional analysis.  This study found that,\non significant market days, there was moderate correlation of events based on the co-\noccurrence graph for notable pairs such as Amazon–JPMorgan, Amazon–Citigroup, Boeing–\nMicrosoft, and Boeing–Amazon.  Specifically, the study found above 60% of the market","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1879,"to":1890}}}}],["3f775727-6c57-4fad-8988-ec0c83783420",{"pageContent":"occurrence graph for notable pairs such as Amazon–JPMorgan, Amazon–Citigroup, Boeing–\nMicrosoft, and Boeing–Amazon.  Specifically, the study found above 60% of the market\nmoves together in these pairs, which aligns with the co-occurrence news network despite\nlower correlation when taken against daily market price alone.  This suggests that the\nco-occurrence graph created using the annotation of articles could be useful for predicting\nmarket movements of notable pairs of companies.\nThis is an important signal for driving better temporal prediction based on the impact\nof financial news and investor sentiment. Future research will include additional signals\nsuch as futures and international money movement, as well as a shorter rolling window\nfor financial emotional analysis to create the co-occurrence graph. This will better align\nwith quarterly financial reporting and produce stronger market event correlation.   By\ncombining the co-occurrence network with time-series analysis and additional market","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1890,"to":1901}}}}],["b40ae215-795b-4b2d-86b3-c6f2ecd7b603",{"pageContent":"with quarterly financial reporting and produce stronger market event correlation.   By\ncombining the co-occurrence network with time-series analysis and additional market\nsignals, a better understanding of macro forces as they relate to market events can be\ngained.  It is important to note that this study has limitations,  and further research is\nneeded to investigate the categorization of emotions over binary polarity and the impact\nof co-occurrence over different and shorter time frames.  Furthermore, emotions in text\nare  analyzed  using  aggregate  data  from  US  and  European  news  articles,  recognizing\nthe complexity of emotions (ethical considerations) the findings should not be used to\ndetermine the emotional state of writers or readers.\nAuthor Contributions:Conceptualization, S.M. and G.A.; methodology, S.M.; software, S.M.; valida-\ntion, S.M. and G.A.; formal analysis, S.M.; investigation, S.M.; resources, S.M.; data curation, S.M.;","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1901,"to":1911}}}}],["cfb296c7-0702-49f4-8bf3-2e9e5e001f5f",{"pageContent":"Author Contributions:Conceptualization, S.M. and G.A.; methodology, S.M.; software, S.M.; valida-\ntion, S.M. and G.A.; formal analysis, S.M.; investigation, S.M.; resources, S.M.; data curation, S.M.;\nwriting—original draft preparation, S.M.; writing—review and editing, G.A.; visualization, S.M.;\nsupervision, G.A. All authors have read and agreed to the published version of the manuscript.\nFunding:This research received no external funding.\nData Availability Statement:The data presented in this study are available on request from the\ncorresponding author. The data are not publicly available due to the NRC Word–Emotion Association\nLexicon only being available to research and educational institutions.\nConflicts of Interest:The authors declare no conflict of interest.\nAppendix A\nEthical Considerations\nEmotions are complex; Microsoft recently deprecated their emotional analysis of faces\ndue to the immense variability in how humans’ express emotions. There are several ethical","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1911,"to":1923}}}}],["a2544cfa-4876-4ecf-9457-58e4407792d2",{"pageContent":"Ethical Considerations\nEmotions are complex; Microsoft recently deprecated their emotional analysis of faces\ndue to the immense variability in how humans’ express emotions. There are several ethical\nconsiderations to consider in NLP analysis of emotions in text.\n•The lexicons and corpus provide a mathematical representation of the aggregate\nemotional tone of a body of text; it should not then follow that the reader experiences\nthe impact as noted; we draw correlation because articles hold the same language and\ntone and therefore are similar.\n•The analysis and observation drawn in this paper are based on aggregate news articles\nacross multiple sources in the US and Europe and we do not draw any conclusions\nbased on individuals’ perception of a particular news source or any one individual’s\nemotional experience to a news article.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1923,"to":1934}}}}],["f2e6e2e8-70b8-4c6a-83cf-0b55d145aca2",{"pageContent":"J. Risk Financial Manag.2023,16, 22616 of 19\n•In this body of work, although conveying information about the perceived emotional\nrepresentation of an article, accurately determining the emotional state of the writer or\neven the reader would require additional information and as such should not be used\nto represent the true emotional state of the writer or reader.\n•We do not recommend using this analysis to draw inferences about an individual or\neven an individual news source, unless 1. it is exercised with caution, 2. the news and\nindividuals consent to the analysis, and 3. an expert in psychology or clinical study\nis included.\nAppendix B\nBelow are the specific concepts queried against the aggregated new sources; these\nrepresent the top 10 companies in each sector based on the percent of holding that company\nrepresents in the sector.\nTable A1.Top 10 company holdings for sectors.\nSectorCompany\nEnergy\nExxon,\nChevron Corporation,\nConoco,\nEOG Resources,\nSchlumberger,\nMarathon Petroleum,","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1936,"to":1957}}}}],["31df254f-f6e5-4578-bb74-9e58beb2accc",{"pageContent":"represents in the sector.\nTable A1.Top 10 company holdings for sectors.\nSectorCompany\nEnergy\nExxon,\nChevron Corporation,\nConoco,\nEOG Resources,\nSchlumberger,\nMarathon Petroleum,\nPioneer Natural Resources,\nPhillips 66,\nKinder Morgan,\nWilliams Companies\nGold\nNewmont Mining Corporation,\nBarrick Gold,\nFranco-Nevada,\nWheaton Precious Metals Corporation,\nNewcrest Mining,\nAgnico Eagle Mines Limited,\nKirkland Lake Gold,\nNorthern Star Resources,\nKinross Gold,\nGold Fields\nMaterials\nThe Linde Group,\nSherwin-Williams,\nAir Products & Chemicals,\nFreeport-McMoRan,\nEcolab,\nNewmont,\nDuPont,\nDow Jones & Company,\nPPG Industries,\nInternational Flavors & Fragrances\nIndustrials\nUnitedHealth Group,\nGoldman Sachs,\nThe Home Depot,\nMicrosoft,\nSalesforce,\nMcDonald’s,\nHoneywell,\nVisa Inc.,\nAmgen,\nBoeing","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":1957,"to":2003}}}}],["d2ec3484-3fc4-41da-8afc-5ff0b1bc60a2",{"pageContent":"J. Risk Financial Manag.2023,16, 22617 of 19\nTable A1.Cont.\nSectorCompany\nConsumer discretionary\nAmazon (company),\nTesla, Inc.,\nThe Home Depot,\nNike, Inc.,\nMcDonald’s,\nLowe’s,\nStarbucks,\nTarget Corporation,\nBooking Holdings,\nTJX Companies\nConsumer staples\nProcter & Gamble,\nThe Coca-Cola Company,\nPepsiCo,\nWalmart,\nCostco,\nPhilip Morris International,\nMondelez International,\nAltria,\nEstée Lauder Companies,\nColgate-Palmolive\nHealthcare\nJohnson & Johnson,\nUnitedHealth Group,\nPfizer,\nAbbott Laboratories,\nAbbVie Inc.,\nThermo Fisher Scientific,\nMerck & Co.,\nEli Lilly and Company,\nDanaher Corporation,\nMedtronic\nfinancials\nBerkshire Hathaway,\nJPMorgan Chase,\nBank of America,\nWells Fargo,\nCitigroup,\nMorgan Stanley,\nGoldman Sachs,\nBlackRock,\nCharles Schwab Corporation,\nAmerican Express\nTechnology\nApple Inc.,\nMicrosoft,\nNvidia,\nVisa Inc.,\nPayPal,\nMastercard,\nAdobe Inc.,\nIntel,\nSalesforce,\nCisco Systems","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2005,"to":2062}}}}],["84e7de63-6b6d-46ba-ac73-19bb545ff771",{"pageContent":"J. Risk Financial Manag.2023,16, 22618 of 19\nTable A1.Cont.\nSectorCompany\nTelecommunications\nCisco Systems,\nVerizon Communications,\nGarmin,\nMotorola Solutions,\nArista Networks,\nAT&T,\nT-Mobile,\nLumen Technologies,\nF5 Networks,\nLiberty Global\nUtilities\nNextEra Energy,\nDuke Energy,\nSouthern Company,\nDominion Energy,\nExelon,\nAmerican Electric Power,\nSempra Energy,\nXcel Energy,\nAmerican Water Works,\nPublic Service Enterprise Group\nReal Estate\nRealty Income Corporation,\nAmerican Tower,\nPrologis,\nCrown Castle International Corp.,\nEquinix,\nPublic Storage,\nDigital Realty,\nSimon Property Group,\nSBA Communications,\nWelltower\nReferences\nAslam, Naila, Furqan Rustam, Ernesto Lee, Patrick Bernard Washington, and Imran Ashraf. 2022. Sentiment analysis and emotion\ndetection on cryptocurrency related tweets using ensemble LSTM-GRU model.IEEE Access10: 39313–24. [CrossRef]\nBird, Steven, Ewan Klein, and Edward Loper. 2009.Natural Language Processing with Python. Sebastopol: O’Reilly Media Inc.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2064,"to":2103}}}}],["9ad287b1-eb5e-4b7e-9bfd-6efdb00d299f",{"pageContent":"Bird, Steven, Ewan Klein, and Edward Loper. 2009.Natural Language Processing with Python. Sebastopol: O’Reilly Media Inc.\nDhar, Suparna, and Indranil Bose.  2020.  Emotions in Twitter communication and stock prices of firms:  The impact of Covid-19\npandemic.Decision47: 385–99. [CrossRef]\nEasterling, ed. 2022. Volatility in Perspective [PDF Document]. Available online: https://www.crestmontresearch.com/docs/Stock-\nVolatility-Perspective.pdf (accessed on 11 October 2022).\nEl-Qadi, Ayoub, Maria Trocan, Thomas Frossard, and Natalia Díaz-Rodríguez. 2022. Credit Risk Scoring Forecasting Using a Time\nSeries Approach.Physical Sciences Forum5: 16. [CrossRef]\nGupta, Aman, Amey Band, Shivam Sharma, and Karan Bilakhiya. 2021. text2emotion-library [Computer Software]. Available online:\nhttps://github.com/aman2656/text2emotion-library (accessed on 24 April 2022).\nHorcher, Karen A. 2011.Essentials of Financial Risk Management. New York: John Wiley & Sons.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2103,"to":2112}}}}],["ab8d92a8-b846-4dce-b668-93cfabfbe3d9",{"pageContent":"https://github.com/aman2656/text2emotion-library (accessed on 24 April 2022).\nHorcher, Karen A. 2011.Essentials of Financial Risk Management. New York: John Wiley & Sons.\nHuang, Martin. 2016. Time Series Analysis for Risk Management in Finance: A Review. Available online: http://www.columbia.edu/\n~mh2078/QRM/TimeSeries_RiskManagement.pdf (accessed on 6 March 2023).\nHutto, Clayton, and Eric Gilbert. 2014. VADER: A parsimonious rule-based model for sentiment analysis of social media text. Paper\npresented at the Eighth International Conference on Weblogs and Social Media (ICWSM-14), Ann Arbor, MI, USA, June 1–4.\nIbbotson, Roger G., and Paul D. Kaplan. 2000. Does asset allocation policy explain 40, 90, or 100 percent of performance?Financial\nAnalysts Journal56: 26–33. [CrossRef]\nLu, Shan, Chenhui Liu, and Zhensong Chen. 2021. Predicting stock market crisis via market indicators and mixed frequency investor\nsentiments.Expert Systems with Applications186: 115844. [CrossRef]","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2112,"to":2121}}}}],["b4225cf9-2b1d-4944-8418-02fa1eb26ce3",{"pageContent":"Lu, Shan, Chenhui Liu, and Zhensong Chen. 2021. Predicting stock market crisis via market indicators and mixed frequency investor\nsentiments.Expert Systems with Applications186: 115844. [CrossRef]\nMazzolli, R., L. Cusick, L. Marcolin, and J. Saperstein. 2023. Learn about Communication Compliance. Microsoft. Available online:\nhttps://learn.microsoft.com/en-us/microsoft-365/compliance/communication-compliance?view=o365-worldwide (accessed\non 9 January 2023).","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2121,"to":2125}}}}],["d2ac25fc-65c6-42a7-ae37-efe3dc2e749a",{"pageContent":"J. Risk Financial Manag.2023,16, 22619 of 19\nPlutchik, Robert.  2001.  The Nature of Emotions:  Human emotions have deep evolutionary roots, a fact that may explain their\ncomplexity and provide tools for clinical practice.American Scientist89: 344–50. [CrossRef]\nRamírez-Sáyago, Ernesto. 2020. Sentiment Analysis from Twitter Data Regarding the COVID-19 Pandemic. Available online: https://\nwww.researchgate.net/publication/346453096_Sentiment_Analysis_from_Twitter_Data_Regarding_the_COVID-19_Pandemic\n(accessed on 3 August 2022).\nShapiro, Adam Hale, Moritz Sudhof, and Daniel Wilson. 2020. Measuring News Sentiment. Federal Reserve Bank of San Francisco,\nWorking Paper Series, (01-49). Available online: https://www.frbsf.org/economic-research/publications/working-papers/2017\n/01/ (accessed on 12 October 2022).\nTang,  Yi,  Yilu Zhou,  and Marshall Hong.   2019.   News Co-Occurrences,  Stock Return Correlations,  and Portfolio Construction","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2127,"to":2136}}}}],["60d78bc7-60eb-4a08-9300-e74b402e3944",{"pageContent":"/01/ (accessed on 12 October 2022).\nTang,  Yi,  Yilu Zhou,  and Marshall Hong.   2019.   News Co-Occurrences,  Stock Return Correlations,  and Portfolio Construction\nImplications.Journal of Risk and Financial Management12: 45. [CrossRef]\nVishnubhotla, Krishnapriya, and Saif M. Mohammad. 2022. Tweet emotion dynamics: Emotion word usage in tweets from US and\nCanada.arXivarXiv:2204.04862. Available online: https://arxiv.org/abs/2204.04862 (accessed on 3 August 2022).\nWan, Xingchen, Jie Yang, Slavi Marinov, Jan-Peter Calliess, Stefan Zohren, and Xiaowen Dong. 2021. Sentiment correlation in financial\nnews networks and associated market movements.Scientific Reports11: 3062. [CrossRef] [PubMed]\nDisclaimer/Publisher’s Note:\nThe statements, opinions and data contained in all publications are solely those of the individual\nauthor(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2136,"to":2145}}}}],["e8dc6a29-3cab-4516-8d2a-66c5c8e351f3",{"pageContent":"author(s) and contributor(s) and not of MDPI and/or the editor(s). MDPI and/or the editor(s) disclaim responsibility for any injury to\npeople or property resulting from any ideas, methods, instructions or products referred to in the content.","metadata":{"source":"/Users/shawnmccarthy/gpt4/docs/jrfm-16-00226.pdf","pdf_numpages":19,"loc":{"lines":{"from":2145,"to":2146}}}}]],{"0":"7a049726-0dfa-4a01-9621-6eb3cfc77318","1":"d786d900-bce1-4a2a-b4c0-8cdc88e73fa1","2":"bda1ea7e-9ebc-476b-add8-5ae0489315ff","3":"f2b19fe4-a705-41a8-871f-49dc141355c0","4":"761d2248-e48d-45eb-9d36-b8a1ee84cef6","5":"abeb6ad7-760b-4467-8f24-e83b99524095","6":"48a16b4f-18b6-4ac0-b897-501b8e0a1dd5","7":"7c8bf096-b2bd-4388-a0fb-e7863bec796d","8":"4abe6f95-d31f-46a8-98f9-064341c94d60","9":"516be376-4e64-467e-bd9a-e2f82afb97f6","10":"42e2a2a3-f9d6-43f8-9b24-1e380a8343c9","11":"03974d5a-c19c-4183-a02b-7c6dd4688030","12":"ffe03d6f-3c40-41d9-8af0-9fae8670b99a","13":"9c607ef6-0002-4b83-8f35-dc04e7e01d09","14":"39be5a3b-1467-424e-9334-c629379b16f1","15":"a1db1a54-1e9a-4b9b-acf2-93357b03b352","16":"3f647cf4-133e-4b1c-bf5b-3b9787f8bd04","17":"024ddb2e-1463-4cac-8f5d-19ee68184318","18":"d35b6a59-7bcc-450a-857b-756f4de7dbb4","19":"e361c49a-5ee5-45f6-a16b-aaa885220af0","20":"306f465a-09b6-4bdf-b874-9f0b1f2c5766","21":"6108309e-7162-4f68-a83c-f3f98fa811f6","22":"b49d0d95-4040-44a9-b068-1b436d0b1fa9","23":"3bbdb3cd-0f75-4f71-b099-60ae778ca0d1","24":"c722f7ea-41ee-45c6-a662-39f2b13ff611","25":"f22104e9-252e-4a82-a9c4-49daaef835d6","26":"2ebc50ab-9b61-46b8-8230-068b79a3c4f5","27":"11c399b2-2420-49fe-8ac3-c7313fe30771","28":"aca81436-ca57-419c-91ec-18392f0bda61","29":"78ce4b39-8271-49fd-a336-53dd798d4a1d","30":"5cecc1a4-fa1e-421b-83aa-bb84f6435201","31":"4d01d6e9-2a07-425b-be51-c3b921142804","32":"2c510b05-06d2-4486-90e1-e4ec5c39c495","33":"c1376daf-9322-44e2-81f9-5a2d3ad0e66f","34":"172ceb0f-2477-450f-9b03-8bb7a0e19caf","35":"21be4427-4515-4199-8f72-15b03d90ef3c","36":"af848532-aa78-4867-bb62-6e19d07d2ec5","37":"4d008b4e-444b-439e-9bc4-a6a45332a2ad","38":"4026d5e5-9ced-45dd-82d1-09d39ce68ce5","39":"280816d6-9416-417b-b5fa-d080889554cb","40":"3d94cf9e-fcb1-44cb-b02b-f19dc121d61a","41":"fa7217c9-d739-477c-9f48-90153fcd6918","42":"46d89046-d4cb-4d38-bc0b-12965d207ec7","43":"a0148e0e-5e16-4bec-9a32-ecb2d99343c7","44":"c7546666-0ce2-4918-9493-a530a82b0488","45":"a914af26-a6f7-42b0-81e3-03038e22eb2b","46":"fa963d6e-447a-4ecf-82e0-10bbca1b88ee","47":"95c9f448-34e3-445e-a6c9-cf0174ce5148","48":"138df212-d190-417a-8257-92e42ed9c4b7","49":"b555517e-2cfd-4873-ac14-da9a77e605f6","50":"18144f28-df2f-4709-bb69-0646fb03ff89","51":"1b8b63c4-e0f4-4631-a7de-889a2cff152e","52":"0c5658fb-bbd3-41f8-9b30-800ab5a52e44","53":"9abbee71-1770-4a93-ae10-ecc9188567b2","54":"97480978-9984-450d-b010-b1a7f53dac5c","55":"a12241b9-2b14-4223-bcc6-5081274fe410","56":"730c5e3d-6258-4320-9d90-c343a6182248","57":"c86fa5a4-ca4b-4414-a27a-8c624cab8f01","58":"74b76c54-12a6-4afb-a410-71d24faa17b2","59":"c59e36fb-23a3-42cc-a550-6c6508e7ca19","60":"20e0fa65-8b0f-4516-a2e6-fbdc063510f0","61":"78a3984c-4f7e-4ad2-9d88-e63038f7e475","62":"221c9c89-97e8-49f6-9e0c-3c3c1af804d4","63":"17f88840-aff6-44bb-96e3-45305643f6c7","64":"3450d0ad-bd1f-48a5-b09f-6c546f7405b0","65":"daa85e1f-d581-42a8-bef6-851f398ccd0f","66":"eed62c54-a778-45e2-9193-de92ba181f77","67":"96992f6e-cbfd-4e67-a088-12bafc7645c6","68":"3ae3ee80-7ca7-4e82-b784-2bddd1b55157","69":"5b2dad8f-0641-4c61-a9e2-79abb12ba39c","70":"7cbd7c68-d90c-4e7f-ac66-b1b5abc58af8","71":"e9f4e1dc-21b0-4aae-a5e1-bea0c52aa9b6","72":"4c4f9d74-ffd4-4232-a9ab-00fabedffef9","73":"81e691ad-8268-4809-bc84-3f18761553c1","74":"bef7ab9a-59b8-4ba8-8a3b-7739dede7750","75":"adb223c2-20f4-42e1-a353-6b93da97d22f","76":"10a2bce3-0bcc-4db4-870e-b6ec33ee7971","77":"7be564ed-85cf-4589-926d-0def0e98d7ce","78":"04b144b3-568e-4660-8bde-99325d460a38","79":"9de5accf-9cc0-4b6c-8f17-c4c0babc5828","80":"36c36f1f-82d6-47d2-9b13-4108cd6c5bc3","81":"ace98a91-aae8-4a9c-bc56-0a1f9e63a0fa","82":"74c934e9-05ac-451b-a7ae-9ba2e76d3588","83":"28fdb179-7006-4468-89eb-e0e6e3f330aa","84":"993c4ee9-2321-4333-a171-8be5ee8f6d18","85":"16702eb6-e3db-4fb6-8c69-3da265138e31","86":"c5ce8613-3302-4200-98d9-aa3796a4caf3","87":"33fe27d9-e89a-475c-ab4f-e502c9d4b814","88":"06d78afb-e2b4-42eb-888a-3dc7e0aa9c21","89":"43cc72fe-6de0-478e-bcb5-d720b5d68e7e","90":"c2c65ead-81f5-4019-970a-9e9b06699071","91":"96149917-9bc3-4e0c-923f-b3803d5f26f1","92":"be9c38d7-c238-4376-9ab8-b631666a086f","93":"1f79b99e-090d-4a50-bd6b-dc33776d5071","94":"ede0f88f-0bed-4dbd-acd1-81c23c8a5d06","95":"d0a04b5f-4150-4135-95aa-477a33012c94","96":"11a21d25-3fca-4e95-a943-07ed6e59010e","97":"f35b15d3-84a9-4c25-aa41-b179f57d35ab","98":"78a9f746-37f4-430a-b8f1-8f83b3d20239","99":"5da04c49-64ab-49ec-a213-3c8fc517a515","100":"83beaa3c-43b0-44e4-92be-49a72d37201a","101":"f439a696-61f7-4382-a762-a42d4e00fa8e","102":"66ff62a2-60e7-49dd-982c-8b51ba3b42b3","103":"66e99bd6-14d5-4d3d-98b1-c627dacf6e6c","104":"e6214bf3-d435-4c39-8d85-42d11570dddf","105":"c4d36444-b389-4732-be1c-43d888d2c6b2","106":"e94e8dff-da8c-4da4-a394-40d4f83fd2b0","107":"220cf824-74d7-462e-83a2-a9a64a2d208c","108":"89b641e0-376f-4fa7-9ff5-236e7a8f436e","109":"fc5c4731-1a5b-421c-98cf-36d4a42c4702","110":"62cacde8-9fcf-4e06-b245-68bbe4507ca8","111":"6d587719-d0bf-4969-9941-aad76e5c0b1a","112":"151dc5aa-50ff-4ab0-8727-b9feedf9b81f","113":"5a7be64f-806a-4715-925d-d21e53147ed4","114":"6b8e6956-e98a-4f3b-8f3a-3fc28df9e598","115":"145070ff-d03d-4c53-a021-2bd6811cf38b","116":"853124be-a59c-4069-8df3-7f6fd7eb152a","117":"1fcdeba2-226c-4b75-8407-30d4a8dbfb28","118":"edb89f17-37cf-41f2-9299-e69c63f4fcf0","119":"b2487806-2766-4dfb-9b2d-df937e54cd7a","120":"f09b7e1f-2218-4a5f-8952-d031df35034b","121":"d217b542-8cf6-4e72-96cd-236020c7c117","122":"fa32b564-b031-4ce7-92c2-db755542801e","123":"0013a3a3-a4f5-47da-904f-c741258bae2e","124":"49ba0b1d-6374-410d-98bb-365ba6bed016","125":"06caf45c-7584-4cb9-be4f-8cb566b3a649","126":"a87c5278-7ada-4d57-97dc-bd9d062f3fc6","127":"88f667c8-89c9-489b-a6b4-a9a0fc891c48","128":"f2502ea5-7fd5-4203-ac45-e8c371dcf291","129":"75b38e82-00e3-4cb2-8a39-59b0dbca7895","130":"c4cb8841-23dd-42a9-b745-d3aeb530b5e7","131":"c033825e-02e1-4e84-8364-68b3f9ac8d1a","132":"3f2c1ee5-7b3d-403a-8a53-7c6a47760a11","133":"96bd9e57-bc57-4e8f-b721-83b9e79ab850","134":"0fa08afa-246e-4f32-9296-d1c0de706538","135":"9b59e3a0-62a7-4267-a77a-eccca87b81f9","136":"b9b40340-28e4-4e3d-9de1-26016ae944e8","137":"07ba5c95-3970-4903-8ed2-a528ac370ed6","138":"26006ccb-eabd-4c82-a995-7006c84b3337","139":"3f775727-6c57-4fad-8988-ec0c83783420","140":"b40ae215-795b-4b2d-86b3-c6f2ecd7b603","141":"cfb296c7-0702-49f4-8bf3-2e9e5e001f5f","142":"a2544cfa-4876-4ecf-9457-58e4407792d2","143":"f2e6e2e8-70b8-4c6a-83cf-0b55d145aca2","144":"31df254f-f6e5-4578-bb74-9e58beb2accc","145":"d2ec3484-3fc4-41da-8afc-5ff0b1bc60a2","146":"84e7de63-6b6d-46ba-ac73-19bb545ff771","147":"9ad287b1-eb5e-4b7e-9bfd-6efdb00d299f","148":"ab8d92a8-b846-4dce-b668-93cfabfbe3d9","149":"b4225cf9-2b1d-4944-8418-02fa1eb26ce3","150":"d2ac25fc-65c6-42a7-ae37-efe3dc2e749a","151":"60d78bc7-60eb-4a08-9300-e74b402e3944","152":"e8dc6a29-3cab-4516-8d2a-66c5c8e351f3"}]